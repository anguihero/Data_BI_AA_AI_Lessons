{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e095100d",
   "metadata": {},
   "source": [
    "\n",
    "# Aprendizaje por Refuerzo (Reinforcement Learning)\n",
    "\n",
    "El **Aprendizaje por Refuerzo (RL)** es una rama del aprendizaje automático en la que un agente aprende a tomar decisiones óptimas mediante la interacción con un entorno, maximizando una señal de recompensa.\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamentos del RL\n",
    "\n",
    "- **Agente**: quien toma decisiones.\n",
    "- **Entorno (Environment)**: donde actúa el agente.\n",
    "- **Estado (State)**: situación actual del entorno.\n",
    "- **Acción (Action)**: decisión del agente.\n",
    "- **Recompensa (Reward)**: señal que recibe el agente tras actuar.\n",
    "- **Política (Policy)**: estrategia de acción del agente.\n",
    "- **Función de valor (Value Function)**: estima la utilidad futura esperada.\n",
    "\n",
    "---\n",
    "\n",
    "## Ecuación de Bellman (Valor Esperado):\n",
    "\n",
    "\\[\n",
    "V(s) = \\mathbb{E}[R_{t+1} + \\gamma V(s') | s]\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- \\( \\gamma \\): factor de descuento.\n",
    "- \\( s' \\): nuevo estado.\n",
    "- \\( R_{t+1} \\): recompensa inmediata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccba494",
   "metadata": {},
   "source": [
    "\n",
    "## Estado del Arte en RL\n",
    "\n",
    "Hoy en día, el aprendizaje por refuerzo ha sido exitosamente aplicado en múltiples áreas:\n",
    "\n",
    "- **Juegos**: AlphaGo, Dota 2, StarCraft II\n",
    "- **Robótica**: control de brazos robóticos\n",
    "- **Finanzas**: estrategias de inversión\n",
    "- **Vehículos autónomos**: planificación y navegación\n",
    "- **Sistemas de recomendación**: decisiones dinámicas\n",
    "\n",
    "---\n",
    "\n",
    "### Algoritmos comunes\n",
    "\n",
    "| Categoría         | Algoritmos                  |\n",
    "|-------------------|-----------------------------|\n",
    "| Basados en valores| Q-Learning, SARSA           |\n",
    "| Basados en políticas | REINFORCE, PPO, A3C        |\n",
    "| Actor-Critic      | DDPG, TD3, A3C              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efeaca0",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo 1: Navegación en una cuadrícula (Grid World)\n",
    "\n",
    "El agente se mueve en una cuadrícula con obstáculos y aprende a llegar al objetivo con la mayor recompensa posible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definición simple de entorno tipo grid\n",
    "grid = np.array([\n",
    "    [-1, -1, -1, +10],\n",
    "    [-1, None, -1, -10],\n",
    "    [-1, -1, -1, -1]\n",
    "])\n",
    "\n",
    "# Posiciones válidas\n",
    "states = [(i, j) for i in range(3) for j in range(4) if grid[i, j] is not None]\n",
    "\n",
    "actions = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "\n",
    "def move(state, action):\n",
    "    i, j = state\n",
    "    di, dj = actions[action]\n",
    "    new_state = (i + di, j + dj)\n",
    "    if new_state in states:\n",
    "        return new_state\n",
    "    return state\n",
    "\n",
    "# Inicializar Q\n",
    "Q = {(s, a): 0 for s in states for a in actions}\n",
    "\n",
    "# Q-learning simple\n",
    "alpha, gamma, epsilon = 0.1, 0.9, 0.1\n",
    "for episode in range(500):\n",
    "    state = (2, 0)\n",
    "    while state not in [(0, 3), (1, 3)]:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(list(actions))\n",
    "        else:\n",
    "            action = max(actions, key=lambda a: Q[state, a])\n",
    "        next_state = move(state, action)\n",
    "        reward = grid[next_state]\n",
    "        best_next = max(Q[next_state, a] for a in actions)\n",
    "        Q[state, action] += alpha * (reward + gamma * best_next - Q[state, action])\n",
    "        state = next_state\n",
    "\n",
    "# Mostrar valores aprendidos\n",
    "for s in sorted(states):\n",
    "    best = max(actions, key=lambda a: Q[s, a])\n",
    "    print(f\"Mejor acción en {s}: {best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c087556",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo 2: Aprendizaje con OpenAI Gym (CartPole)\n",
    "\n",
    "El agente aprende a mantener una barra en equilibrio sobre un carro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e766bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "obs = env.reset()[0]\n",
    "\n",
    "for _ in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()[0]\n",
    "env.close()\n",
    "print(\"Ejemplo de interacción aleatoria completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50ff45",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo 3: Multi-Armed Bandit\n",
    "\n",
    "Simulación de toma de decisiones con varias opciones (máquinas tragamonedas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3 máquinas con distintas probabilidades\n",
    "probs = [0.2, 0.5, 0.8]\n",
    "n = 1000\n",
    "counts = [0] * 3\n",
    "values = [0.0] * 3\n",
    "\n",
    "def pull(arm):\n",
    "    return np.random.rand() < probs[arm]\n",
    "\n",
    "for t in range(1, n + 1):\n",
    "    epsilon = 0.1\n",
    "    if np.random.rand() < epsilon:\n",
    "        arm = np.random.choice(3)\n",
    "    else:\n",
    "        arm = np.argmax(values)\n",
    "    reward = pull(arm)\n",
    "    counts[arm] += 1\n",
    "    values[arm] += (reward - values[arm]) / counts[arm]\n",
    "\n",
    "plt.bar(range(3), values)\n",
    "plt.title(\"Valor estimado por máquina\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a543c6",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo 4: Control de temperatura inteligente\n",
    "\n",
    "Simulación de RL para encender/apagar calefacción en función del confort y ahorro energético.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estados: temperaturas\n",
    "temps = list(range(15, 26))  # 15°C a 25°C\n",
    "actions = ['ON', 'OFF']\n",
    "Q = {(t, a): 0 for t in temps for a in actions}\n",
    "\n",
    "for _ in range(1000):\n",
    "    t = np.random.choice(temps)\n",
    "    a = np.random.choice(actions)\n",
    "    reward = -abs(22 - t) if a == 'OFF' else -1  # confort o costo energético\n",
    "    best = max(Q[t, a2] for a2 in actions)\n",
    "    Q[t, a] += 0.1 * (reward + 0.9 * best - Q[t, a])\n",
    "\n",
    "for t in temps:\n",
    "    best_a = max(actions, key=lambda a: Q[t, a])\n",
    "    print(f\"A {t}°C => Acción óptima: {best_a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422bb35",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo 5: Estrategia de compra/venta (RL simplificado)\n",
    "\n",
    "El agente aprende cuándo comprar, vender o mantener una acción en función de una señal de mercado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prices = np.sin(np.linspace(0, 20, 100)) + np.random.normal(0, 0.1, 100)\n",
    "actions = ['buy', 'sell', 'hold']\n",
    "Q = {(i, a): 0 for i in range(len(prices)) for a in actions}\n",
    "inventory = []\n",
    "\n",
    "for _ in range(100):\n",
    "    for i in range(len(prices) - 1):\n",
    "        a = np.random.choice(actions)\n",
    "        reward = 0\n",
    "        if a == 'buy':\n",
    "            inventory.append(prices[i])\n",
    "        elif a == 'sell' and inventory:\n",
    "            reward = prices[i] - inventory.pop(0)\n",
    "        Q[i, a] += 0.01 * (reward - Q[i, a])\n",
    "\n",
    "# Mostrar mejores acciones aprendidas\n",
    "for i in range(10):\n",
    "    print(f\"T={i}, Mejor acción:\", max(actions, key=lambda a: Q[i, a]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5fbda",
   "metadata": {},
   "source": [
    "\n",
    "## Aprendizaje por Refuerzo Profundo (Deep Reinforcement Learning)\n",
    "\n",
    "El **Deep Reinforcement Learning (DRL)** combina el aprendizaje por refuerzo con redes neuronales profundas, permitiendo al agente aprender políticas directamente desde observaciones complejas como imágenes, texto o vectores continuos de alta dimensión.\n",
    "\n",
    "---\n",
    "\n",
    "### Librería recomendada: `stable-baselines3`\n",
    "\n",
    "```bash\n",
    "pip install stable-baselines3[extra]\n",
    "```\n",
    "\n",
    "Modelos disponibles:\n",
    "- `DQN` (Deep Q-Network)\n",
    "- `PPO` (Proximal Policy Optimization)\n",
    "- `A2C` (Advantage Actor-Critic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9c747",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo: PPO con `CartPole-v1` (estable y robusto)\n",
    "\n",
    "Entrenaremos un agente con `stable-baselines3` usando PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb82501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Crear entorno vectorizado\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "\n",
    "# Entrenar modelo\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=5000)\n",
    "print(\"Entrenamiento PPO completo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c13500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "print(\"Evaluación completada.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
