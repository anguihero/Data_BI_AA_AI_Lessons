{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7e591c",
   "metadata": {},
   "source": [
    "\n",
    "# Redes Neuronales Recurrentes (RNN, LSTM, GRU)\n",
    "\n",
    "Las **Redes Neuronales Recurrentes (RNN)** están diseñadas para trabajar con **secuencias** de datos, como texto, audio, series de tiempo y más.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué usar RNN?\n",
    "\n",
    "- Capturan dependencias temporales.\n",
    "- Mantienen una memoria del pasado.\n",
    "- Ideales para tareas como predicción de texto, análisis de sentimiento, traducción y más.\n",
    "\n",
    "---\n",
    "\n",
    "## Tipos de redes recurrentes:\n",
    "\n",
    "| Tipo  | Descripción                                      |\n",
    "|-------|--------------------------------------------------|\n",
    "| RNN   | Básica, pero con dificultad para memorias largas |\n",
    "| LSTM  | Incluye compuertas para manejar memoria a largo plazo |\n",
    "| GRU   | Variante más eficiente del LSTM                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Crear una secuencia simple de números\n",
    "data = np.array([i for i in range(100)])\n",
    "X = []\n",
    "y = []\n",
    "n_steps = 5\n",
    "\n",
    "for i in range(len(data) - n_steps):\n",
    "    X.append(data[i:i+n_steps])\n",
    "    y.append(data[i+n_steps])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))  # [samples, timesteps, features]\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359de5cd",
   "metadata": {},
   "source": [
    "\n",
    "## Arquitectura RNN Simple\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "```\n",
    "\n",
    "Se procesa una secuencia paso a paso. Es simple, pero tiende a olvidar información lejana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b465ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model_rnn = Sequential([\n",
    "    SimpleRNN(50, activation='tanh', input_shape=(n_steps, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='mse')\n",
    "model_rnn.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "print(\"Predicción:\", model_rnn.predict(X[:5]).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed876d75",
   "metadata": {},
   "source": [
    "\n",
    "## LSTM (Long Short-Term Memory)\n",
    "\n",
    "Usa compuertas (entrada, olvido, salida) para aprender dependencias a largo plazo.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LSTM\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, activation='tanh', input_shape=(n_steps, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "model_lstm.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "print(\"Predicción:\", model_lstm.predict(X[:5]).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2e268",
   "metadata": {},
   "source": [
    "\n",
    "## GRU (Gated Recurrent Unit)\n",
    "\n",
    "Red más rápida que LSTM, con compuertas combinadas.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import GRU\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(50, activation='tanh', input_shape=(n_steps, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='mse')\n",
    "model_gru.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "print(\"Predicción:\", model_gru.predict(X[:5]).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977ea54",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Crea una nueva serie secuencial como una onda senoidal con ruido.\n",
    "2. Compara el rendimiento de RNN, LSTM y GRU.\n",
    "3. Prueba `return_sequences=True` y apilar múltiples capas.\n",
    "4. Cambia la longitud de la secuencia (`n_steps`).\n",
    "\n",
    "¿Observas diferencias de desempeño entre las arquitecturas?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}