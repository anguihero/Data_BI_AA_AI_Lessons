{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab55312b",
   "metadata": {},
   "source": [
    "\n",
    "# Fundamentos Esenciales de Machine Learning\n",
    "\n",
    "Este notebook está diseñado para comprender conceptos esenciales en Machine Learning. Nos enfocaremos en tres temas fundamentales:\n",
    "\n",
    "1. **Gradiente Descendente**\n",
    "2. **Sobreajuste (Overfitting)**\n",
    "3. **Complejidad del Modelo y Trade-off (Bias-Variance Trade-off)**\n",
    "\n",
    "---\n",
    "\n",
    "Estos conceptos son clave para entender cómo aprenden los modelos y cómo podemos optimizar su rendimiento en problemas reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718616e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Gradiente Descendente\n",
    "\n",
    "El **gradiente descendente** es un algoritmo de optimización usado para minimizar funciones de costo. En el contexto de ML, se utiliza para minimizar el error entre las predicciones del modelo y los valores reales.\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "1. Calcula la derivada (gradiente) de la función de pérdida.\n",
    "2. Da un paso en dirección opuesta al gradiente.\n",
    "3. Repite hasta llegar a un mínimo (idealmente global).\n",
    "\n",
    "### Fórmula:\n",
    "\\[\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- \\( \\theta \\): parámetros del modelo\n",
    "- \\( \\alpha \\): tasa de aprendizaje (learning rate)\n",
    "- \\( J(\\theta) \\): función de pérdida\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulación simple de gradiente descendente para minimizar y = x^2\n",
    "x = 10\n",
    "learning_rate = 0.1\n",
    "history = []\n",
    "\n",
    "for _ in range(20):\n",
    "    grad = 2 * x\n",
    "    x = x - learning_rate * grad\n",
    "    history.append(x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history, marker='o')\n",
    "plt.title('Convergencia del Gradiente Descendente')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Valor de x')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35ff69",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Sobreajuste (Overfitting)\n",
    "\n",
    "El **sobreajuste** ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo ruido o patrones irrelevantes, perdiendo capacidad de generalización.\n",
    "\n",
    "### Señales de sobreajuste:\n",
    "- Muy bajo error en entrenamiento.\n",
    "- Alto error en validación/test.\n",
    "\n",
    "### Comparación:\n",
    "\n",
    "|                | Entrenamiento | Test |\n",
    "|----------------|----------------|------|\n",
    "| Modelo ideal   | Bajo error     | Bajo error |\n",
    "| Sobreajustado  | Muy bajo error | Alto error |\n",
    "\n",
    "### Ejemplo visual:\n",
    "- Un modelo muy complejo se ajusta a todos los puntos.\n",
    "- Un modelo simple captura la tendencia general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d957e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Datos de ejemplo\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(20, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n",
    "\n",
    "# Modelos con diferente complejidad\n",
    "degrees = [1, 15]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i, deg in enumerate(degrees):\n",
    "    model = make_pipeline(PolynomialFeatures(deg), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.scatter(X, y, color='black')\n",
    "    plt.plot(X, y_pred, color='blue', label=f\"Grado {deg}\")\n",
    "    plt.title(f\"Modelo de grado {deg}\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b3abb",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Complejidad del Modelo y Trade-off\n",
    "\n",
    "Existe un equilibrio entre **bias (sesgo)** y **varianza** al diseñar un modelo:\n",
    "\n",
    "- **Modelo muy simple (subajuste)**: alto bias, baja varianza.\n",
    "- **Modelo muy complejo (sobreajuste)**: bajo bias, alta varianza.\n",
    "\n",
    "Este equilibrio se llama **trade-off bias-varianza**.\n",
    "\n",
    "### Gráfico típico:\n",
    "\n",
    "- El error total se compone de bias², varianza y ruido.\n",
    "- Hay un punto óptimo de complejidad donde el error es mínimo.\n",
    "\n",
    "### ¿Qué hacer?\n",
    "- Usar validación cruzada para elegir la complejidad ideal.\n",
    "- Regularizar los modelos (L1, L2).\n",
    "- Tener más datos para estabilizar modelos complejos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulación del trade-off\n",
    "complexity = np.arange(1, 20)\n",
    "bias = (1 / complexity)\n",
    "variance = np.log(complexity) / 5\n",
    "error_total = bias**2 + variance\n",
    "\n",
    "plt.plot(complexity, bias**2, label='Bias²')\n",
    "plt.plot(complexity, variance, label='Varianza')\n",
    "plt.plot(complexity, error_total, label='Error Total')\n",
    "plt.xlabel('Complejidad del Modelo')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Trade-off Bias-Varianza')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b6872",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Implementa un modelo de regresión polinomial de grado 3 sobre los datos de ejemplo y grafica los resultados.\n",
    "2. Ajusta el tamaño del `learning_rate` en el ejemplo de gradiente descendente y observa los cambios.\n",
    "3. ¿Qué pasa si el modelo polinomial tiene grado 50? ¿Hay sobreajuste?\n",
    "\n",
    "¡Explora, experimenta y observa los resultados!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}