{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0181d8a7",
   "metadata": {},
   "source": [
    "\n",
    "# Reducción de la Dimensionalidad: Técnicas y Aplicaciones\n",
    "\n",
    "La **reducción de la dimensionalidad** es una técnica fundamental cuando trabajamos con conjuntos de datos con muchas variables. Permite:\n",
    "\n",
    "- Eliminar ruido y redundancia.\n",
    "- Visualizar datos en 2D o 3D.\n",
    "- Mejorar el rendimiento de los modelos.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos:\n",
    "\n",
    "1. Entender la motivación y fundamentos matemáticos.\n",
    "2. Aplicar técnicas como **PCA**, **MFA**, **t-SNE** y **UMAP**.\n",
    "3. Usar `scikit-learn` y visualizar resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e370c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset Iris\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec843568",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Análisis de Componentes Principales (PCA)\n",
    "\n",
    "### ¿Qué es PCA?\n",
    "\n",
    "PCA busca nuevas variables (componentes principales) que sean combinaciones lineales de las variables originales, maximizando la varianza.\n",
    "\n",
    "---\n",
    "\n",
    "### Matemáticamente:\n",
    "\n",
    "1. Estandarizar los datos.\n",
    "2. Calcular matriz de covarianza:  \n",
    "\\[\n",
    "\\Sigma = \\frac{1}{n} X^T X\n",
    "\\]\n",
    "3. Calcular autovalores y autovectores de \\(\\Sigma\\).\n",
    "4. Elegir los k vectores propios más grandes.\n",
    "5. Transformar los datos:\n",
    "\\[\n",
    "X_{\\text{reducido}} = X W_k\n",
    "\\]\n",
    "\n",
    "Donde \\(W_k\\) contiene los vectores propios seleccionados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Estandarizar\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Visualizar\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA - Componentes principales')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4cd50",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Análisis de Factores Múltiples (MFA)\n",
    "\n",
    "El **MFA** es una extensión del PCA para **datos estructurados en grupos** (bloques de variables).\n",
    "\n",
    "- Se aplica un PCA en cada grupo.\n",
    "- Se ponderan los datos según la inercia.\n",
    "- Se obtiene una representación global equilibrada.\n",
    "\n",
    "Es útil cuando tenemos variables de diferentes tipos o grupos lógicos, como:\n",
    "- Opiniones (bloque 1)\n",
    "- Comportamientos (bloque 2)\n",
    "- Datos demográficos (bloque 3)\n",
    "\n",
    "```bash\n",
    "pip install prince\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d57fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import prince\n",
    "\n",
    "# Ejemplo simple: simular 2 bloques\n",
    "df = X.copy()\n",
    "df['grupo1'] = y  # Simular un bloque adicional\n",
    "\n",
    "mfa = prince.MFA(\n",
    "    groups={'medidas': 4, 'grupo1': 1},\n",
    "    n_components=2,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "mfa = mfa.fit(df)\n",
    "mfa.plot_row_coordinates(df, ax=None, figsize=(6, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ebb4a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Otras Técnicas: t-SNE y UMAP\n",
    "\n",
    "### t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "- Preserva distancias locales.\n",
    "- Útil para visualizar datos complejos no lineales.\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "- Más rápido que t-SNE en grandes conjuntos.\n",
    "- Preserva tanto estructura global como local.\n",
    "\n",
    "```python\n",
    "import umap\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_std)\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_std)\n",
    "\n",
    "# Visualización comparativa\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='Spectral')\n",
    "axs[0].set_title(\"t-SNE\")\n",
    "\n",
    "axs[1].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='Spectral')\n",
    "axs[1].set_title(\"UMAP\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ffe78",
   "metadata": {},
   "source": [
    "\n",
    "## Comparación: PCA vs UMAP\n",
    "\n",
    "| Característica                 | PCA                                | UMAP                                 |\n",
    "|-------------------------------|-------------------------------------|--------------------------------------|\n",
    "| Tipo de reducción             | Lineal                              | No lineal                            |\n",
    "| Preservación de estructura    | Global                              | Local y global                       |\n",
    "| Interpretabilidad             | Alta (combinaciones lineales)       | Baja (espacio proyectado no lineal) |\n",
    "| Sensible a escalado           | Sí                                  | Sí                                   |\n",
    "| Velocidad                     | Muy rápida                          | Rápida (más lenta que PCA)          |\n",
    "| Visualización                 | Buena para datos lineales           | Excelente para estructuras complejas|\n",
    "| Dependencia de hiperparámetros| No (solo n_components)              | Sí (n_neighbors, min_dist, etc.)    |\n",
    "| Uso típico                    | Preprocesamiento, explicación       | Visualización, descubrimiento       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62919bf5",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Usa el dataset `load_wine()` o cualquier otro de alta dimensión.\n",
    "2. Aplica PCA, t-SNE y UMAP.\n",
    "3. Compara visualmente las representaciones reducidas.\n",
    "4. ¿Qué técnica conserva mejor la separación entre clases?\n",
    "\n",
    "¿Notas diferencias en velocidad, dispersión o agrupamientos?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
