{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d24b1b",
   "metadata": {},
   "source": [
    "\n",
    "# Interpretabilidad de Modelos de Machine Learning\n",
    "\n",
    "La interpretabilidad es la capacidad de entender por qué un modelo toma una decisión determinada. Es clave para:\n",
    "\n",
    "- Tomar decisiones informadas.\n",
    "- Garantizar la equidad.\n",
    "- Detectar errores.\n",
    "- Cumplir con regulaciones.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos del notebook:\n",
    "\n",
    "1. Comprender distintas técnicas de interpretabilidad.\n",
    "2. Aplicar métodos globales y locales.\n",
    "3. Analizar la importancia de características, contribuciones individuales, y visualizaciones.\n",
    "4. Introducir `SHAP`, `Permutation Importance`, `Partial Dependence`, `ALE`, y más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar datos\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "modelo = RandomForestRegressor(random_state=42)\n",
    "modelo.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c3705",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importancia de Características (Global)\n",
    "\n",
    "Los modelos de árbol permiten calcular la **importancia global** de cada variable:\n",
    "\n",
    "- Basada en reducción del error (impureza).\n",
    "- Mide qué tanto mejora la predicción al usar una variable.\n",
    "\n",
    "Sin embargo, puede estar sesgada hacia variables con muchos niveles o alta cardinalidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "importancias = pd.Series(modelo.feature_importances_, index=X.columns).sort_values()\n",
    "importancias.plot(kind='barh', figsize=(8, 5), title=\"Importancia Global de Características\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cec36",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Permutation Importance (Global)\n",
    "\n",
    "Evalúa la importancia de una variable **perturbando** sus valores y midiendo la caída en desempeño.\n",
    "\n",
    "- Más fiable que la importancia basada en árboles.\n",
    "- No depende de la estructura del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(modelo, X_test, y_test, n_repeats=10, random_state=42)\n",
    "perm_df = pd.Series(result.importances_mean, index=X.columns).sort_values()\n",
    "\n",
    "perm_df.plot(kind='barh', figsize=(8, 5), title=\"Permutation Importance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f595c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gráficos de Dependencia Parcial (PDP)\n",
    "\n",
    "Visualizan el **efecto promedio** de una característica en la predicción.\n",
    "\n",
    "- Buena para interpretabilidad global.\n",
    "- Asume independencia entre variables.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "PartialDependenceDisplay.from_estimator(modelo, X, features=[\"LSTAT\"], ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1dcf3",
   "metadata": {},
   "source": [
    "\n",
    "## 5. SHAP: SHapley Additive exPlanations\n",
    "\n",
    "Descompone la predicción de cada observación en **contribuciones de cada característica**, usando teoría de juegos.\n",
    "\n",
    "### Ventajas:\n",
    "\n",
    "- Coherente y localmente exacto.\n",
    "- Compatible con modelos arbitrarios (`KernelExplainer`, `TreeExplainer`).\n",
    "\n",
    "```bash\n",
    "pip install shap\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(modelo, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP summary plot\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b8723",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Cambia el modelo a un `GradientBoostingRegressor` o `XGBoost`.\n",
    "2. Aplica `PartialDependenceDisplay` y `SHAP` a dos variables simultáneamente.\n",
    "3. Usa `shap.plots.waterfall()` para analizar un caso individual.\n",
    "\n",
    "¿Puedes explicar qué variables impulsaron una predicción hacia arriba o hacia abajo?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}