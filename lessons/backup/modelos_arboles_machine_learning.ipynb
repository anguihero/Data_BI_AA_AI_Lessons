{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427b827f",
   "metadata": {},
   "source": [
    "\n",
    "# Modelos Basados en Árboles\n",
    "\n",
    "Los modelos basados en árboles son una clase poderosa de algoritmos de machine learning que funcionan dividiendo recursivamente el espacio de entrada en regiones más pequeñas.\n",
    "\n",
    "### Modelos incluidos:\n",
    "- Árboles de decisión (Decision Trees)\n",
    "- Bosques aleatorios (Random Forest)\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "Estos modelos se usan tanto para **clasificación** como para **regresión**.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué usar modelos basados en árboles?\n",
    "\n",
    "- No requieren normalización de datos.\n",
    "- Capturan relaciones no lineales.\n",
    "- Soportan datos mixtos (numéricos y categóricos).\n",
    "- Son fáciles de interpretar (especialmente los árboles simples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16cb18",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Árboles de Decisión\n",
    "\n",
    "Un **árbol de decisión** construye un modelo en forma de árbol donde cada nodo representa una pregunta (condición), y las hojas representan predicciones.\n",
    "\n",
    "### Hiperparámetros principales:\n",
    "- `max_depth`: Profundidad máxima del árbol.\n",
    "- `min_samples_split`: Número mínimo de muestras para dividir un nodo.\n",
    "- `min_samples_leaf`: Muestras mínimas en una hoja.\n",
    "- `criterion`: Función para medir la calidad de una división (`gini`, `entropy` o `squared_error`).\n",
    "\n",
    "### Ventajas:\n",
    "- Fácil interpretación.\n",
    "- Bajo costo computacional.\n",
    "\n",
    "### Desventajas:\n",
    "- Tienden al sobreajuste si no se podan.\n",
    "- Alta varianza.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be385e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, criterion='gini')\n",
    "tree.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(tree, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names)\n",
    "plt.title(\"Árbol de Decisión (max_depth=3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee230ada",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Bosques Aleatorios (Random Forest)\n",
    "\n",
    "Un **Random Forest** construye múltiples árboles de decisión y promedia sus predicciones para reducir la varianza.\n",
    "\n",
    "### Hiperparámetros importantes:\n",
    "- `n_estimators`: Número de árboles.\n",
    "- `max_features`: Número de características consideradas en cada división.\n",
    "- `bootstrap`: Si se usan muestras con reemplazo.\n",
    "\n",
    "### Ventajas:\n",
    "- Reduce el sobreajuste respecto a un solo árbol.\n",
    "- Más preciso en muchos problemas.\n",
    "\n",
    "### Desafíos:\n",
    "- Menor interpretabilidad que un solo árbol.\n",
    "- Más costoso computacionalmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Precisión del Random Forest:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17565a59",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "El **Gradient Boosting** construye árboles secuencialmente, cada uno intentando corregir los errores del anterior.\n",
    "\n",
    "### Hiperparámetros clave:\n",
    "- `n_estimators`: Número de árboles.\n",
    "- `learning_rate`: Tasa de aprendizaje (cuánto corrige cada árbol).\n",
    "- `max_depth`: Profundidad de cada árbol.\n",
    "- `subsample`: Porcentaje de datos usado por árbol.\n",
    "\n",
    "### Ventajas:\n",
    "- Gran precisión si se ajustan correctamente.\n",
    "- Funciona bien en datos estructurados/tabulares.\n",
    "\n",
    "### Desafíos:\n",
    "- Requiere ajuste fino de hiperparámetros.\n",
    "- Mayor tiempo de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Precisión de Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3982c",
   "metadata": {},
   "source": [
    "\n",
    "## Consideraciones Finales\n",
    "\n",
    "| Modelo             | Precisión | Interpretabilidad | Sobreajuste | Tiempo de entrenamiento |\n",
    "|--------------------|-----------|-------------------|-------------|--------------------------|\n",
    "| Árbol de decisión  | Media     | Alta              | Alto        | Bajo                     |\n",
    "| Random Forest      | Alta      | Media             | Bajo        | Medio                    |\n",
    "| Gradient Boosting  | Muy Alta  | Baja              | Medio       | Alto                     |\n",
    "\n",
    "### Recomendaciones:\n",
    "- Usa árboles simples si necesitas interpretabilidad.\n",
    "- Usa Random Forest para buena precisión sin tanto tuning.\n",
    "- Usa Boosting para máxima performance (con tuning).\n",
    "\n",
    "---\n",
    "\n",
    "## Ejercicio Sugerido\n",
    "\n",
    "Entrena un Random Forest y un Gradient Boosting sobre el dataset `wine` de sklearn. Compara sus desempeños y ajusta `max_depth` y `n_estimators`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}