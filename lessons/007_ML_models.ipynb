{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf4EI65f2pfw"
      },
      "source": [
        "# Modelos Analiticos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uINmJItuASxv"
      },
      "source": [
        "## Ciclo de un proyecto de ciencia de datos / Analítico\n",
        "### CRISP-DM : Cross-Industry Standard Process for Data Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBu0AdoqASxw"
      },
      "source": [
        "![CRISP-DM](https://github.com/elprincipitogauss/lesson/blob/main/crispdm_process.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkS6z4bTASxw"
      },
      "source": [
        "\n",
        "\n",
        "**Comprensión del Negocio (Business Understanding)**: En esta etapa inicial, se trabaja en estrecha colaboración con los stakeholders del proyecto para comprender los objetivos comerciales y las necesidades que se deben abordar. Se definen los objetivos de la minería de datos y se establece un plan de proyecto.\n",
        "\n",
        "**Comprensión de los Datos (Data Understanding)**: Aquí, se recopilan y exploran los datos disponibles para el proyecto. Esto implica la recopilación de datos, la identificación de fuentes de datos, la evaluación de la calidad de los datos y la realización de análisis exploratorios para comprender mejor la naturaleza de los datos.\n",
        "\n",
        "**Preparación de los Datos (Data Preparation)**: En esta etapa, se preparan los datos para su posterior análisis. Esto incluye la limpieza de datos, la transformación de datos, la selección de variables relevantes y la creación de conjuntos de datos adecuados para el modelado.\n",
        "\n",
        "**Modelado (Modeling)**: Aquí es donde se aplican técnicas de modelado de datos, como algoritmos de aprendizaje automático, para construir modelos predictivos o descriptivos. Se prueban diferentes enfoques y se ajustan los modelos para obtener los mejores resultados posibles.\n",
        "\n",
        "**Evaluación (Evaluation)**: En esta etapa, se evalúan los modelos creados en la etapa de modelado. Se utilizan métricas de rendimiento y técnicas de validación cruzada para determinar la calidad y la idoneidad de los modelos. Si los modelos no cumplen con los criterios de éxito, se pueden realizar ajustes en las etapas anteriores.\n",
        "\n",
        "**Despliegue (Deployment)**: Una vez que se han seleccionado los modelos finales, se implementan en un entorno de producción. Esto puede implicar la integración de los modelos en sistemas empresariales, la automatización de procesos o la creación de informes interactivos para los usuarios finales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaQdgIiBASxw"
      },
      "source": [
        "## Etapas de un problema de machine learning\n",
        "\n",
        "* Definir el problema: ¿Qué se pretende predecir? ¿De qué datos se dispone? o ¿Qué datos es necesario conseguir?\n",
        "\n",
        "* Explorar y entender los datos que se van a emplear para crear el modelo.\n",
        "\n",
        "* Métrica de éxito: definir una forma apropiada de cuantificar cómo de buenos son los resultados obtenidos.\n",
        "\n",
        "* Preparar la estrategia para evaluar el modelo: separar las observaciones en un conjunto de entrenamiento, un conjunto de validación (o validación cruzada) y un conjunto de test. Es muy importante asegurar que ninguna información del conjunto de test participa en el proceso de entrenamiento del modelo.\n",
        "\n",
        "* Preprocesar los datos: aplicar las transformaciones necesarias para que los datos puedan ser interpretados por el algoritmo de machine learning seleccionado.\n",
        "\n",
        "* Ajustar un primer modelo capaz de superar unos resultados mínimos. Por ejemplo, en problemas de clasificación, el mínimo a superar es el porcentaje de la clase mayoritaria (la moda). En un modelo de regresión, la media de la variable respuesta.\n",
        "\n",
        "* Gradualmente, mejorar el modelo incorporando-creando nuevas variables u optimizando los hiperparámetros.\n",
        "\n",
        "* Evaluar la capacidad del modelo final con el conjunto de test para tener una estimación de la capacidad que tiene el modelo cuando predice nuevas observaciones.\n",
        "\n",
        "* Entrenar el modelo final con todos los datos disponibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71dzNQJhcdeY"
      },
      "source": [
        "## Preparacion de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lJnUnKb2nFM"
      },
      "outputs": [],
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns #\n",
        "import statsmodels.api as sm\n",
        "# mirar librerias dash plotly (objetos visuales intercativos y crear aplicaciones web HTML)\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# from skopt.space import Real, Integer\n",
        "# from skopt.utils import use_named_args\n",
        "# from skopt import gp_minimize\n",
        "# from skopt.plots import plot_convergence\n",
        "# import optuna\n",
        "\n",
        "# Varios\n",
        "# ==============================================================================\n",
        "import multiprocessing #\n",
        "import random\n",
        "from itertools import product\n",
        "# from fitter import Fitter, get_common_distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtMMiqXXASxy"
      },
      "outputs": [],
      "source": [
        "# Configuración matplotlib graficas\n",
        "# ==============================================================================\n",
        "plt.rcParams['image.cmap'] = \"bwr\"\n",
        "#plt.rcParams['figure.dpi'] = \"100\"\n",
        "plt.rcParams['savefig.bbox'] = \"tight\"\n",
        "style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JocweMW_ASxy"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH5XkPaYASxy"
      },
      "source": [
        "El set de datos *SaratogaHouses* del paquete mosaicData de R contiene información sobre el precio de 1728 viviendas situadas en Saratoga County, New York, USA en el año 2006. Además del precio, incluye 15 variables adicionales:\n",
        "\n",
        "* price: precio de la vivienda.\n",
        "* lotSize: metros cuadrados de la vivienda.\n",
        "* age: antigüedad de la vivienda.\n",
        "* landValue: valor del terreno.\n",
        "* livingArea: metros cuadrados habitables.\n",
        "* pctCollege: porcentaje del vecindario con título universitario.\n",
        "* bedrooms: número de dormitorios.\n",
        "* firplaces: número de chimeneas.\n",
        "* bathrooms: número de cuartos de baño (el valor 0.5 hace referencia a cuartos de baño sin ducha).\n",
        "* rooms: número de habitaciones.\n",
        "* heating: tipo de calefacción.\n",
        "* fuel: tipo de alimentación de la calefacción (gas, electricidad o diesel).\n",
        "* sewer: tipo de desagüe.\n",
        "* waterfront: si la vivienda tiene vistas al lago.\n",
        "* newConstruction: si la vivienda es de nueva construcción.\n",
        "* centralAir: si la vivienda tiene aire acondicionado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLbNA2NmASxy"
      },
      "outputs": [],
      "source": [
        "# en una direccion web url se encuentra el archivo\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/\"\n",
        "    \"master/data/SaratogaHouses.csv\"\n",
        ")\n",
        "# lea de la direccion el archivo\n",
        "datos = pd.read_csv(url, sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "drIh2DriASxy",
        "outputId": "f7cd4e0d-f91a-46e7-e300-f43e504da69a"
      },
      "outputs": [],
      "source": [
        "# revisar que datos se leyeron (qué y cómo?)\n",
        "datos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diwH4JRtASxz",
        "outputId": "28b6d8b6-dd57-4b99-c86a-1682f8f583f9"
      },
      "outputs": [],
      "source": [
        "datos.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sflxpeSASxz"
      },
      "outputs": [],
      "source": [
        "# definir un nuevo nombre de columnas\n",
        "listados_nuevo_nombre_cols = [\"precio\", \"metros_totales\", \"antiguedad\", \"precio_terreno\", \"metros_habitables\",\n",
        "                 \"universitarios\", \"dormitorios\", \"chimenea\", \"banyos\", \"habitaciones\",\n",
        "                 \"calefaccion\",\"consumo_calefacion\", \"desague\", \"vistas_lago\", \"nueva_construccion\",\n",
        "                 \"aire_acondicionado\"]\n",
        "\n",
        "# Se renombran las columnas para que sean más descriptivas\n",
        "datos.columns = listados_nuevo_nombre_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amf8zOK6ASxz",
        "outputId": "a5e2c905-b9fb-45cf-c916-ec1746376f20"
      },
      "outputs": [],
      "source": [
        "datos.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifRIxwDAASx0"
      },
      "source": [
        "## exploratorio : EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0n1Vn7NASx0"
      },
      "source": [
        "Este proceso permite entender mejor qué información contiene cada variable, así como detectar posibles errores. Algunos ejemplos frecuentes son:\n",
        "\n",
        "* Que una columna se haya almacenado con el tipo incorrecto: una variable numérica está siendo reconocida como texto o viceversa.\n",
        "* Que una variable contenga valores que no tienen sentido: por ejemplo, para indicar que no se dispone del precio de una vivienda se introduce el valor 0 o un espacio en blanco.\n",
        "* Que en una variable de tipo numérico se haya introducido una palabra en lugar de un número."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "X-RPqmPBASx0",
        "outputId": "b2176528-b94d-4e12-a851-b02a1d6e6f56"
      },
      "outputs": [],
      "source": [
        "datos.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "684-_ccoASx0",
        "outputId": "89c5a9ca-d41e-4e4c-a56c-eed8a9f04598"
      },
      "outputs": [],
      "source": [
        "# Tipo de cada columna\n",
        "# ==============================================================================\n",
        "# En pandas, el tipo \"object\" hace referencia a strings\n",
        "# datos.dtypes\n",
        "datos.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4zmO0ZQASx0",
        "outputId": "ed03637e-8e58-42bc-b5d4-7b546e955ae5"
      },
      "outputs": [],
      "source": [
        "# Dimensiones del dataset\n",
        "# ==============================================================================\n",
        "datos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hCGLt9BASx0",
        "outputId": "a34641c8-45e2-44ae-f4f3-9f050b46ddf9"
      },
      "outputs": [],
      "source": [
        "# Número de datos ausentes por variable\n",
        "# ==============================================================================\n",
        "datos.isna().sum().sort_values() # indicar que columnas necesitan estrategia de tratamiento de vacios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qECVnvwASx1"
      },
      "source": [
        "### Variable respuesta (Endogena : variable a explicar dentro del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "-305CS-KASx1",
        "outputId": "dabdd86d-ccbb-4aec-d77b-f5333943ed7e"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 6))\n",
        "sns.kdeplot(\n",
        "    datos.precio,\n",
        "    fill    = True,\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[0]\n",
        ")\n",
        "sns.rugplot(\n",
        "    datos.precio,\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Distribución original\", fontsize = 'medium')\n",
        "axes[0].set_xlabel('precio', fontsize='small')\n",
        "axes[0].tick_params(labelsize = 6)\n",
        "\n",
        "sns.kdeplot(\n",
        "    np.sqrt(datos.precio),\n",
        "    fill    = True,\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[1]\n",
        ")\n",
        "sns.rugplot(\n",
        "    np.sqrt(datos.precio),\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Transformación raíz cuadrada\", fontsize = 'medium')\n",
        "axes[1].set_xlabel('sqrt(precio)', fontsize='small')\n",
        "axes[1].tick_params(labelsize = 6)\n",
        "\n",
        "sns.kdeplot(\n",
        "    np.log(datos.precio),\n",
        "    fill    = True,\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[2]\n",
        ")\n",
        "sns.rugplot(\n",
        "    np.log(datos.precio),\n",
        "    color   = \"blue\",\n",
        "    ax      = axes[2]\n",
        ")\n",
        "axes[2].set_title(\"Transformación logarítmica\", fontsize = 'medium')\n",
        "axes[2].set_xlabel('log(precio)', fontsize='small')\n",
        "axes[2].tick_params(labelsize = 6)\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNrVqrP1ASx1"
      },
      "source": [
        "Algunos modelos de machine learning y aprendizaje estadístico requieren que la variable respuesta se distribuya de una forma determinada. Por ejemplo, para los modelos de regresión lineal (LM), la distribución tiene que ser de tipo normal. Para los modelos lineales generalizados (GLM), la distribución tiene que ser de la familia exponencial.\n",
        "\n",
        "Existen varias librerías en python que permiten identificar a qué distribución se ajustan mejor los datos, una de ellas es fitter. Esta librería permite ajustar cualquiera de las 80 distribuciones implementadas en scipy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvch0bsKASx1"
      },
      "outputs": [],
      "source": [
        "# distribuciones = ['cauchy', 'chi2', 'expon',  'exponpow', 'gamma',\n",
        "#                   'norm', 'powerlaw', 'beta', 'logistic']\n",
        "\n",
        "# fitter = Fitter(datos.precio, distributions=distribuciones)\n",
        "# fitter.fit()\n",
        "# fitter.summary(Nbest=10, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgt3yLRTASx1"
      },
      "source": [
        "### Variables numéricas (Exogenas :  variables con las que se pretende explicar la variable endogena)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rN-rE7AWASx1",
        "outputId": "24b18595-1d37-44b9-e501-582f9592a11a"
      },
      "outputs": [],
      "source": [
        "# Variables numéricas\n",
        "# ==============================================================================\n",
        "datos.select_dtypes(include=['float64', 'int']).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "lnuxRgQcASx2",
        "outputId": "aa4ee1b5-cd05-494d-fe2e-ce7cd6a16d4b"
      },
      "outputs": [],
      "source": [
        "# Gráfico de distribución para cada variable numérica\n",
        "# ==============================================================================\n",
        "# Ajustar número de subplots en función del número de columnas\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
        "axes = axes.flat\n",
        "columnas_numeric = datos.select_dtypes(include=['float64', 'int']).columns\n",
        "columnas_numeric = columnas_numeric.drop('precio')\n",
        "\n",
        "for i, colum in enumerate(columnas_numeric):\n",
        "    sns.histplot(\n",
        "        data     = datos,\n",
        "        x        = colum,\n",
        "        stat     = \"count\",\n",
        "        kde      = True,\n",
        "        color    = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\n",
        "        line_kws = {'linewidth': 2},\n",
        "        alpha    = 0.3,\n",
        "        ax       = axes[i]\n",
        "    )\n",
        "    axes[i].set_title(colum, fontsize = 7, fontweight = \"bold\")\n",
        "    axes[i].tick_params(labelsize = 6)\n",
        "    axes[i].set_xlabel(\"\")\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(top = 0.9)\n",
        "fig.suptitle('Distribución variables numéricas', fontsize = 10, fontweight = \"bold\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNHy8P8RASx2"
      },
      "source": [
        "La variable chimenea, aunque es de tipo numérico, apenas toma unos pocos valores y la gran mayoría de observaciones pertenecen a solo dos de ellos. En casos como este, suele ser conveniente tratar la variable como cualitativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCIoch0HASx2",
        "outputId": "5a7888cc-b849-4bb3-8311-f18e4c03e394"
      },
      "outputs": [],
      "source": [
        "# Valores observados de chimenea\n",
        "# ==============================================================================\n",
        "datos.chimenea.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V71O4JDwASx2"
      },
      "outputs": [],
      "source": [
        "# Se convierte la variable chimenea tipo string\n",
        "# ==============================================================================\n",
        "datos.chimenea = datos.chimenea.astype(\"str\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNbsHyYGASx2"
      },
      "source": [
        "Como el objetivo del estudio es predecir el precio de las viviendas, el análisis de cada variable se hace también en relación a la variable respuesta precio. Analizando los datos de esta forma, se pueden empezar a extraer ideas sobre qué variables están más relacionadas con el precio y de qué forma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "ceWUA34AASx2",
        "outputId": "3ad8ea25-9a24-4678-87fa-510a871243b0"
      },
      "outputs": [],
      "source": [
        "# Gráfico de distribución para cada variable numérica\n",
        "# ==============================================================================\n",
        "# Ajustar número de subplots en función del número de columnas\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
        "axes = axes.flat\n",
        "columnas_numeric = datos.select_dtypes(include=['float64', 'int']).columns\n",
        "columnas_numeric = columnas_numeric.drop('precio')\n",
        "\n",
        "for i, colum in enumerate(columnas_numeric):\n",
        "    sns.regplot(\n",
        "        x           = datos[colum],\n",
        "        y           = datos['precio'],\n",
        "        color       = \"gray\",\n",
        "        marker      = '.',\n",
        "        scatter_kws = {\"alpha\":0.4},\n",
        "        line_kws    = {\"color\":\"r\",\"alpha\":0.7},\n",
        "        ax          = axes[i]\n",
        "    )\n",
        "    axes[i].set_title(f\"precio vs {colum}\", fontsize = 7, fontweight = \"bold\")\n",
        "    #axes[i].ticklabel_format(style='sci', scilimits=(-4,4), axis='both')\n",
        "    axes[i].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "    axes[i].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "    axes[i].tick_params(labelsize = 6)\n",
        "    axes[i].set_xlabel(\"\")\n",
        "    axes[i].set_ylabel(\"\")\n",
        "\n",
        "# Se eliminan los axes vacíos\n",
        "for i in [8]:\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "fig.suptitle('Correlación con precio', fontsize = 10, fontweight = \"bold\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn23-36-ASx3"
      },
      "source": [
        "### Correlación variables numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPkEjnSpASx3"
      },
      "source": [
        "Algunos modelos (LM, GLM, ...) se ven perjudicados si incorporan predictores altamente correlacionados. Por esta razón, es conveniente estudiar el grado de correlación entre las variables disponibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WrFMVG24ASx3",
        "outputId": "b654ac1c-3168-417a-e786-3abe79ef6b18"
      },
      "outputs": [],
      "source": [
        "# Correlación entre columnas numéricas\n",
        "# ==============================================================================\n",
        "def tidy_corr_matrix(corr_mat):\n",
        "    '''\n",
        "    Función para convertir una matrix de correlación de pandas en formato tidy\n",
        "    '''\n",
        "    corr_mat = corr_mat.stack().reset_index()\n",
        "    corr_mat.columns = ['variable_1','variable_2','r']\n",
        "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
        "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
        "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
        "\n",
        "    return(corr_mat)\n",
        "\n",
        "corr_matrix = datos.select_dtypes(include=['float64', 'int']).corr(method='pearson')\n",
        "tidy_corr_matrix(corr_matrix).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "aj9h-HKQASx3",
        "outputId": "a5741321-72f3-4b9f-e64e-1856ed8e9ceb"
      },
      "outputs": [],
      "source": [
        "# Heatmap matriz de correlaciones\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot     = True,\n",
        "    cbar      = False,\n",
        "    annot_kws = {\"size\": 6},\n",
        "    vmin      = -1,\n",
        "    vmax      = 1,\n",
        "    center    = 0,\n",
        "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
        "    square    = True,\n",
        "    ax        = ax\n",
        ")\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right',\n",
        ")\n",
        "\n",
        "ax.tick_params(labelsize = 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyXzQmmYASx3"
      },
      "source": [
        "### Variables cualitativas (Exogenas :  variables con las que se pretende explicar la variable endogena)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "7G0i48qYASx3",
        "outputId": "593dd180-3825-456a-a235-3e4d8a2ea63f"
      },
      "outputs": [],
      "source": [
        "# Variables cualitativas (tipo object)\n",
        "# ==============================================================================\n",
        "datos.select_dtypes(include=['object']).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "Ca-DlhDxASx7",
        "outputId": "5029d0ee-c4af-426f-c056-188e523ea344"
      },
      "outputs": [],
      "source": [
        "# Gráfico para cada variable cualitativa\n",
        "# ==============================================================================\n",
        "# Ajustar número de subplots en función del número de columnas\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
        "axes = axes.flat\n",
        "columnas_object = datos.select_dtypes(include=['object']).columns\n",
        "\n",
        "for i, colum in enumerate(columnas_object):\n",
        "    datos[colum].value_counts().plot.barh(ax = axes[i])\n",
        "    axes[i].set_title(colum, fontsize = 7, fontweight = \"bold\")\n",
        "    axes[i].tick_params(labelsize = 6)\n",
        "    axes[i].set_xlabel(\"\")\n",
        "\n",
        "# Se eliminan los axes vacíos\n",
        "for i in [7, 8]:\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "fig.suptitle('Distribución variables cualitativas',\n",
        "             fontsize = 10, fontweight = \"bold\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxOUIkw7ASx7"
      },
      "source": [
        "Si alguno de los niveles de una variable cualitativa tiene muy pocas observaciones en comparación a los otros niveles, puede ocurrir que, durante la validación cruzada o bootstrapping, algunas particiones no contengan ninguna observación de dicha clase (varianza cero), lo que puede dar lugar a errores. En estos casos, suele ser conveniente:\n",
        "\n",
        "* Eliminar las observaciones del grupo minoritario si es una variable multiclase.\n",
        "* Eliminar la variable si solo tiene dos niveles.\n",
        "* Agrupar los niveles minoritarios en un único grupo.\n",
        "* Asegurar que, en la creación de las particiones, todos los grupos estén representados en cada una de ellas.\n",
        "\n",
        "Para este caso, hay que tener precaución con la variable chimenea. Se unifican los niveles de 2, 3 y 4 en un nuevo nivel llamado \"2_mas\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Sa9ESoASx7",
        "outputId": "6523c026-89df-4837-d7dd-dd06288662bd"
      },
      "outputs": [],
      "source": [
        "datos.chimenea.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3Q5zR9AASx8"
      },
      "outputs": [],
      "source": [
        "# diccionario con los casos a reemplazar\n",
        "dic_replace = {'2': \"2_mas\",\n",
        "               '3': \"2_mas\",\n",
        "               '4': \"2_mas\"}\n",
        "\n",
        "datos['chimenea'] = (\n",
        "    datos['chimenea']\n",
        "    .map(dic_replace)\n",
        "    .fillna(datos['chimenea'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3blw94bIASx8",
        "outputId": "475fda88-ae6c-428e-fe06-43d2e57e2a3e"
      },
      "outputs": [],
      "source": [
        "datos.chimenea.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "apjePJiGASx8",
        "outputId": "cc290ff5-b6fb-4792-858a-0190a91d8c39"
      },
      "outputs": [],
      "source": [
        "# Gráfico relación entre el precio y cada cada variables cualitativas\n",
        "# ==============================================================================\n",
        "# Ajustar número de subplots en función del número de columnas\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 5))\n",
        "axes = axes.flat\n",
        "columnas_object = datos.select_dtypes(include=['object']).columns\n",
        "\n",
        "for i, colum in enumerate(columnas_object):\n",
        "    sns.violinplot(\n",
        "        x     = colum,\n",
        "        y     = 'precio',\n",
        "        data  = datos,\n",
        "        color = \"white\",\n",
        "        ax    = axes[i]\n",
        "    )\n",
        "    axes[i].set_title(f\"precio vs {colum}\", fontsize = 7, fontweight = \"bold\")\n",
        "    axes[i].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "    axes[i].tick_params(labelsize = 6)\n",
        "    axes[i].set_xlabel(\"\")\n",
        "    axes[i].set_ylabel(\"\")\n",
        "\n",
        "# Se eliminan los axes vacíos\n",
        "for i in [7, 8]:\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "fig.suptitle('Distribución del precio por grupo', fontsize = 10, fontweight = \"bold\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726O9PjHASx8"
      },
      "source": [
        "## Metodos de Regresion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qNctb88ASx8"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfaqdVOASx8"
      },
      "source": [
        "### División train y test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF6Ylp-OASx8"
      },
      "source": [
        "Evaluar la capacidad predictiva de un modelo consiste en comprobar cómo de próximas son sus predicciones a los verdaderos valores de la variable respuesta. Para poder cuantificarlo de forma correcta, se necesita disponer de un conjunto de observaciones, de las que se conozca la variable respuesta, pero que el modelo no haya \"visto\", es decir, que no hayan participado en su ajuste. Con esta finalidad, se dividen los datos disponibles en un conjunto de entrenamiento y un conjunto de test. El tamaño adecuado de las particiones depende en gran medida de la cantidad de datos disponibles y la seguridad que se necesite en la estimación del error, 80%-20% suele dar buenos resultados. El reparto debe hacerse de forma aleatoria o aleatoria-estratificada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3CLQqPzASx9"
      },
      "outputs": [],
      "source": [
        "# Reparto de datos en train y test\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 70train-30test % o 80train-20test%\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                        datos.drop('precio', axis = 'columns'), # X exogenas\n",
        "                                        datos['precio'], # y endogena\n",
        "                                        train_size   = 0.8, # % de datos para la particion de datos de entrenamiento\n",
        "                                        random_state = 1234, # semilla aleatoria\n",
        "                                        shuffle      = True\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGNmjix7ASx9"
      },
      "source": [
        "Es importante verificar que la distribución de la variable respuesta es similar en el conjunto de entrenamiento y en el de test. Para asegurar que esto se cumple, la función train_test_split() de scikit-learn permite, en problemas de clasificación, identificar con el argumento stratify la variable en base a la cual hacer el reparto.\n",
        "\n",
        "Este tipo de reparto estratificado asegura que el conjunto de entrenamiento y el de test sean similares en cuanto a la variable respuesta, sin embargo, no garantiza que ocurra lo mismo con los predictores. Por ejemplo, en un set de datos con 100 observaciones, un predictor binario que tenga 90 observaciones de un grupo y solo 10 de otro, tiene un alto riesgo de que, en alguna de las particiones, el grupo minoritario no tenga representantes. Si esto ocurre en el conjunto de entrenamiento, algunos algoritmos darán error al aplicarlos al conjunto de test, ya que no entenderán el valor que se les está pasando. Este problema puede evitarse eliminando variables con varianza próxima a cero (ver más adelante)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilwT5cuGASx9",
        "outputId": "1b1c75cb-48bc-480e-85c2-a5e3ab1639d0"
      },
      "outputs": [],
      "source": [
        "print(\"Partición de entrenamento\")\n",
        "print(\"-----------------------\")\n",
        "print(y_train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBx17oxvASx9",
        "outputId": "2a308331-e4ee-4e5a-f140-5a572a580805"
      },
      "outputs": [],
      "source": [
        "print(\"Partición de test\")\n",
        "print(\"-----------------------\")\n",
        "print(y_test.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9QDfsl_ASx9"
      },
      "source": [
        "### Preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwsHiPsASx9"
      },
      "source": [
        "El preprocesado engloba todas aquellas transformaciones realizadas sobre los datos con el objetivo que puedan ser interpretados por el algoritmo de machine learning lo más eficientemente posible. Todo preprocesado de datos debe aprenderse con las observaciones de entrenamiento y luego aplicarse al conjunto de entrenamiento y al de test. Esto es muy importante para no violar la condición de que ninguna información procedente de las observaciones de test participe o influya en el ajuste del modelo. Este principio debe aplicarse también si se emplea validación cruzada (ver más adelante). En tal caso, el preprocesado debe realizarse dentro de cada iteración de validación, para que las estimaciones que se hacen con cada partición de validación no contengan información del resto de particiones. Aunque no es posible crear un único listado, a continuación se resumen algunos de los pasos de preprocesado que más se suelen necesitar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mDmogIASx-"
      },
      "source": [
        "#### Imputación de valores ausentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNgmM0d_ASx-"
      },
      "source": [
        "La gran mayoría de algoritmos no aceptan observaciones incompletas, por lo que, cuando el set de datos contiene valores ausentes, se puede:\n",
        "\n",
        "* Eliminar aquellas observaciones que estén incompletas.\n",
        "* Eliminar aquellas variables que contengan valores ausentes.\n",
        "* Tratar de estimar los valores ausentes empleando el resto de información disponible (imputación).\n",
        "\n",
        "Las primeras dos opciones, aunque sencillas, suponen perder información. La eliminación de observaciones solo puede aplicarse cuando se dispone de muchas y el porcentaje de registros incompletos es muy bajo. En el caso de eliminar variables, el impacto dependerá de cuánta información aporten dichas variables al modelo. Cuando se emplea la imputación, es muy importante tener en cuenta el riesgo que se corre al introducir valores en predictores que tengan mucha influencia en el modelo. Supóngase un estudio médico en el que, cuando uno de los predictores es positivo, el modelo predice casi siempre que el paciente está sano. Para un paciente cuyo valor de este predictor se desconoce, el riesgo de que la imputación sea errónea es muy alto, por lo que es preferible obtener una predicción basada únicamente en la información disponible. Esta es otra muestra de la importancia que tiene que el analista conozca el problema al que se enfrenta y pueda así tomar la mejor decisión.\n",
        "\n",
        "El módulo sklearn.impute incorpora varios métodos de imputación distintos:\n",
        "\n",
        "* SimpleImputer: permite imputaciones empleando un valor constante o un estadístico (media, mediana, valor más frecuente) de la misma columna en la que se encuentra el valor ausente.\n",
        "* IterativeImputer: permite imputar el valor de una columna teniendo en cuenta el resto de columnas. En concreto, se trata de un proceso iterativo en el que, en cada iteración, una de las variables se emplea como variable respuesta y el resto como predictores. Una vez obtenido el modelo, se emplea para predecir las posiciones vacías de esa variable. Este proceso se lleva a cabo con cada variable y se repite el ciclo max_iter veces para ganar estabilidad. La implementación de sklearn.impute.IterativeImputer permite que se emplee casi cualquiera de sus algoritmos para crear los modelos de imputación (KNN, RandomForest, GradientBoosting...).\n",
        "* KNNImputer: es un caso concreto de IterativeImputer en el que se emplea k-Nearest Neighbors como algoritmo de imputación.\n",
        "\n",
        "A pesar de ser un método muy utilizado, imputar utilizando KNN presenta dos problemas: su coste computacional elevado hace que solo sea aplicable en conjuntos de datos de tamaño pequeño o moderado. Si hay variables categóricas, debido a la dificultad de medir \"distancias\" en este contexto, puede dar lugar a resultados poco realistas. Por estas dos razones, es más recomendable utilizar un modelo tipo Random Forest IterativeImputer(predictor = RandomForestRegressor()).\n",
        "\n",
        "Con el argumento add_indicator=True se crea automáticamente una nueva columna en la que se indica con el valor 1 qué valores han sido imputados. Esto puede ser útil tanto para identificar las observaciones en las que se ha realizado alguna imputación como para utilizarla como un predictor más en el mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5uxR5TQASx-"
      },
      "source": [
        "#### Pipeline y ColumnTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh1whUn2ASx-"
      },
      "source": [
        "Las clases ColumnTransformer y make_column_transformer del módulo sklearn.compose permiten combinar múltiples transformaciones de preprocesado, especificando a qué columnas se aplica cada una. Como todo transformer, tiene un método de entrenamiento (fit) y otro de transformación (transform) . Esto permite que el aprendizaje de las transformaciones se haga únicamente con observaciones de entrenamiento, y se puedan aplicar después a cualquier conjunto de datos. La idea detrás de este módulo es la siguiente:\n",
        "\n",
        "* Definir todas las transformaciones (escalado, selección, filtrado...) que se desea aplicar y a qué columnas ColumnTransformer(). La selección de columnas puede hacerse por: nombre. índice, máscara booleana, slice, patrón regex, por tipo de columna o con las funciones de selección make_column_selector.\n",
        "* Aprender los parámetros necesarios para dichas transformaciones con las observaciones de entrenamiento .fit().\n",
        "* Aplicar las transformaciones aprendidas a cualquier conjunto de datos .transform()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSVIzVVDASx-"
      },
      "outputs": [],
      "source": [
        "# Selección de las variables por tipo\n",
        "# ==============================================================================\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder # dummies para vars categoricas\n",
        "from sklearn.preprocessing import StandardScaler # normalizar o escalar para vars numericas\n",
        "# from sklearn.compose import make_column_selector\n",
        "\n",
        "# Se estandarizan las columnas numéricas y se hace one-hot-encoding de las\n",
        "# columnas cualitativas. Para mantener las columnas a las que no se les aplica\n",
        "# ninguna transformación se tiene que indicar remainder='passthrough'.\n",
        "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list() # vars numericas\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list() # vars categoricas\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "                   [('scale', StandardScaler(), numeric_cols), # vars numericas las voy estadarizar\n",
        "                    ('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)], # las vars categoricas voy onehotencoding (dummies)\n",
        "                remainder='passthrough')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alTtxn4cASx-"
      },
      "source": [
        "Una vez que se ha definido el objeto ColumnTransformer, con el método fit() se aprenden las transformaciones con los datos de entrenamiento y se aplican a los dos conjuntos con transform()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of9hMG2cASx_"
      },
      "outputs": [],
      "source": [
        "X_train_prep = preprocessor.fit_transform(X_train) # entrenar y aplicar\n",
        "X_test_prep  = preprocessor.transform(X_test) # aplicar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxz4UkV2ASx_"
      },
      "source": [
        "Por defecto, el resultado devuelto por ColumnTransformer es un numpy array, por lo que se pierden los nombres de las columnas. Suele ser interesante poder inspeccionar cómo queda el set de datos tras el preprocesado en formato dataframe. Por defecto, OneHotEncoder ordena las nuevas columnas de izquierda a derecha por orden alfabético."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "_034Mdh0ASx_",
        "outputId": "91a35909-f02b-4a6f-dc65-222d0676e7b2"
      },
      "outputs": [],
      "source": [
        "# Convertir el output en dataframe y añadir el nombre de las columnas\n",
        "# ==============================================================================\n",
        "encoded_cat = preprocessor.named_transformers_['onehot'].get_feature_names_out(cat_cols)\n",
        "nombre_columnas = np.concatenate([numeric_cols, encoded_cat])\n",
        "X_train_prep = preprocessor.transform(X_train)\n",
        "X_train_prep = pd.DataFrame(X_train_prep, columns=nombre_columnas)\n",
        "X_test_prep = pd.DataFrame(X_test_prep, columns=nombre_columnas)\n",
        "X_train_prep.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHq8IeC4ASx_"
      },
      "source": [
        "A partir de las versión scikit-learn 0.23 se puede crear una representación interactiva de un objeto pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "YSWKSzBRASx_",
        "outputId": "a817e6cd-9f77-45e3-f74d-bc56782af469"
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "set_config(display='diagram')\n",
        "\n",
        "preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nfg0BJMASx_"
      },
      "source": [
        "### Ajustar un modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaWGHVwxASx_"
      },
      "source": [
        "El siguiente paso tras definir los datos de entrenamiento, es seleccionar el algoritmo que se va a emplear. En scikit-learn, esto se hace mediante la creación de un objeto estimator. En concreto, este objeto almacena el nombre del algoritmo, sus parámetros e hiperparámetros y contiene los métodos fit(X, y) y predict(T) que le permiten aprender de los datos y predecir nuevas observaciones. El siguiente listado contiene todos los algoritmos implementados en scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZpFXu3xASyA"
      },
      "source": [
        "#### entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezPrhmUDASyA"
      },
      "source": [
        "##### regresion lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "FGYkEDX7ASyA",
        "outputId": "ca34c375-d7dc-43b8-a800-46c9d47c991e"
      },
      "outputs": [],
      "source": [
        "# Creación del modelo\n",
        "# ==============================================================================\n",
        "modelo_rl = LinearRegression() # ols : iniciar un objeto de modelo\n",
        "modelo_rl.fit(X_train_prep, y = y_train)  # ajuste de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep883xcIASyA",
        "outputId": "72eca7de-f7ac-4459-ff8d-09288dca93ee"
      },
      "outputs": [],
      "source": [
        "# Información del modelo\n",
        "# ==============================================================================\n",
        "print(\"Intercept:\", modelo_rl.intercept_)\n",
        "print(\"Coeficiente:\", list(zip(datos.columns, modelo_rl.coef_.flatten(), )))\n",
        "print(\"Coeficiente de determinación R^2:\", modelo_rl.score(X_train_prep, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT05_rBZASyA",
        "outputId": "9b9b83f6-3271-49d2-ddf3-da4140cb2298"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "predicciones = modelo_rl.predict(X = X_test_prep) # prediccion\n",
        "print(predicciones[0:3,])\n",
        "\n",
        "rmse = mean_squared_error(\n",
        "        y_true  = y_test,\n",
        "        y_pred  = predicciones,\n",
        "        squared = False\n",
        "       )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {np.sqrt(rmse)}\") # está en las unidades de la var end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kWEck03vASyA",
        "outputId": "ac118b46-08da-490c-a82d-9205e09cd8d2"
      },
      "outputs": [],
      "source": [
        "# Predicciones con intervalo de confianza del 95%\n",
        "# ==============================================================================\n",
        "predicciones = modelo_rl.predict(X_train_prep)\n",
        "observado = y_train\n",
        "\n",
        "# Gráfico del modelo\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "\n",
        "plt.scatter(observado,predicciones)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA_zkGiXASyB"
      },
      "source": [
        "##### arbol de decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ONOjgciJASyB",
        "outputId": "714c9cc5-d480-4991-ba0c-43dfc390c6ac"
      },
      "outputs": [],
      "source": [
        "# Creación del modelo\n",
        "# ------------------------------------------------------------------------------\n",
        "modelo_cart = DecisionTreeRegressor(\n",
        "            max_depth         = 3, # hiperparametros\n",
        "            random_state      = 123\n",
        "          )\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "# ------------------------------------------------------------------------------\n",
        "modelo_cart.fit(X_train_prep, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "t8y4ifz-ASyB",
        "outputId": "473b60e0-98e2-49c6-ea58-378f60c7a045"
      },
      "outputs": [],
      "source": [
        "# Estructura del árbol creado\n",
        "# ------------------------------------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(16, 5))\n",
        "\n",
        "print(f\"Profundidad del árbol: {modelo_cart.get_depth()}\")\n",
        "print(f\"Número de nodos terminales: {modelo_cart.get_n_leaves()}\")\n",
        "\n",
        "plot = plot_tree(\n",
        "            decision_tree = modelo_cart,\n",
        "            feature_names = datos.drop(columns = \"precio\").columns,\n",
        "            class_names   = 'precio',\n",
        "            filled        = True,\n",
        "            impurity      = False,\n",
        "            fontsize      = 10,\n",
        "            precision     = 2,\n",
        "            ax            = ax\n",
        "       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO2FjUfxASyB",
        "outputId": "0ae517ca-d781-46b7-ba7b-26f583632bac"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "predicciones = modelo_cart.predict(X = X_test_prep)\n",
        "print(predicciones[0:3,])\n",
        "\n",
        "rmse = mean_squared_error(\n",
        "        y_true  = y_test,\n",
        "        y_pred  = predicciones,\n",
        "        squared = False\n",
        "       )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {np.sqrt(rmse)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jYBmBSSUASyB",
        "outputId": "b946d3cb-e2d6-4b96-a320-3ce3e4167df1"
      },
      "outputs": [],
      "source": [
        "# Predicciones con intervalo de confianza del 95%\n",
        "# ==============================================================================\n",
        "predicciones = modelo_cart.predict(X_train_prep)\n",
        "observado = y_train\n",
        "\n",
        "# Gráfico del modelo\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "\n",
        "plt.scatter(observado,predicciones)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDMOgGSOASyB"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "2RL81SlFASyC",
        "outputId": "a744c770-e94e-4e20-c247-150ca566e035"
      },
      "outputs": [],
      "source": [
        "# Creación del modelo\n",
        "# ==============================================================================\n",
        "modelo_rf = RandomForestRegressor(\n",
        "            n_estimators = 10, # cantidad de arboles de decision\n",
        "            criterion    = 'friedman_mse', # metrica / perdida\n",
        "            max_depth    = None,\n",
        "            max_features = 'auto',\n",
        "            oob_score    = False,\n",
        "            n_jobs       = -1, # jobs van a ser los nucles en los que puedo paralelizar los modelos (-1 escoge el maximo de executors)\n",
        "            random_state = 123\n",
        "         )\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "# ==============================================================================\n",
        "modelo_rf.fit(X_train_prep, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg04LEuxASyC",
        "outputId": "fc4a21ea-41bf-42fb-ae22-e65a860f1e6b"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "predicciones = modelo_rf.predict(X = X_test_prep)\n",
        "print(predicciones[0:3,])\n",
        "\n",
        "rmse = mean_squared_error(\n",
        "        y_true  = y_test,\n",
        "        y_pred  = predicciones,\n",
        "        squared = False\n",
        "       )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {np.sqrt(rmse)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CKbRkon_ASyC",
        "outputId": "427763cd-02b1-4dcc-8133-6baa37e02bf4"
      },
      "outputs": [],
      "source": [
        "# Predicciones con intervalo de confianza del 95%\n",
        "# ==============================================================================\n",
        "predicciones = modelo_rf.predict(X_train_prep)\n",
        "observado = y_train\n",
        "\n",
        "# Gráfico del modelo\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "\n",
        "plt.scatter(observado,predicciones)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGd7KMLOASyC"
      },
      "source": [
        "#### validacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIW37mSzASyC"
      },
      "source": [
        "##### validacion cruzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "nE-a9YvAASyC",
        "outputId": "86cffe7b-7159-4cf8-d2ff-89392c2a3267"
      },
      "outputs": [],
      "source": [
        "# Validación cruzada repetida con múltiples métricas\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=123)\n",
        "cv_scores = cross_validate(\n",
        "                estimator = DecisionTreeRegressor(),\n",
        "                X         = X_train_prep,\n",
        "                y         = y_train,\n",
        "                scoring   = ('r2', 'neg_root_mean_squared_error'),\n",
        "                cv        = cv,\n",
        "                return_train_score = True\n",
        "            )\n",
        "\n",
        "# Se convierte el diccionario a dataframe para facilitar la visualización\n",
        "cv_scores = pd.DataFrame(cv_scores)\n",
        "cv_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5te-RHLfASyC"
      },
      "source": [
        "##### Grid Search basado en validacion cruzada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJUV6qL9ASyD"
      },
      "source": [
        "* tuneo de hyperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phZ8ZvcqASyD"
      },
      "outputs": [],
      "source": [
        "# Grid de hiperparámetros evaluados\n",
        "# ==============================================================================\n",
        "param_grid = {'n_estimators': [150],\n",
        "              'max_features': [5, 7, 9],\n",
        "              'max_depth'   : [None, 3, 10, 20]\n",
        "             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l3WbId_ASyD"
      },
      "outputs": [],
      "source": [
        "# Búsqueda por grid search con validación cruzada\n",
        "# ==============================================================================\n",
        "grid = GridSearchCV( # grilla de busqueda de hiperparametros\n",
        "        estimator  = RandomForestRegressor(random_state = 123),\n",
        "        param_grid = param_grid,\n",
        "        scoring    = 'r2',\n",
        "        n_jobs     = multiprocessing.cpu_count() - 1, # seleccionar la cantidad de nucleos máxima que diospongo para paralelizar\n",
        "        cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123),\n",
        "        refit      = True,\n",
        "        verbose    = 0,\n",
        "        return_train_score = True\n",
        "       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "w-zo0cHTASyD",
        "outputId": "cc980e57-080f-4c85-ee60-216b529f06f8"
      },
      "outputs": [],
      "source": [
        "# ajuste\n",
        "grid.fit(X = X_train_prep, y = y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "W54kNdpVASyD",
        "outputId": "6d540793-7e75-479e-ff44-7a944ca06d47"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Resultados\n",
        "# ==============================================================================\n",
        "resultados = pd.DataFrame(grid.cv_results_)\n",
        "resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
        "    .drop(columns = 'params') \\\n",
        "    .sort_values('mean_test_score', ascending = False) \\\n",
        "    .head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "lQKefw2VASyD",
        "outputId": "7a9f7508-2281-4336-dc1e-1a4f7c55a52f"
      },
      "outputs": [],
      "source": [
        "# Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
        "# ==============================================================================\n",
        "train_scores = []\n",
        "cv_scores    = []\n",
        "\n",
        "# Valores evaluados\n",
        "estimator_range = range(1, 150, 5)\n",
        "\n",
        "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
        "# de entrenamiento y de k-cross-validation.\n",
        "for n_estimators in estimator_range:\n",
        "\n",
        "    modelo = RandomForestRegressor(\n",
        "                n_estimators = n_estimators,\n",
        "                criterion    = 'friedman_mse',\n",
        "                max_depth    = None,\n",
        "                max_features = 'auto',\n",
        "                oob_score    = False,\n",
        "                n_jobs       = -1,\n",
        "                random_state = 123\n",
        "             )\n",
        "\n",
        "    # Error de train\n",
        "    modelo.fit(X_train_prep, y_train)\n",
        "    predicciones = modelo.predict(X = X_train_prep)\n",
        "    rmse = mean_squared_error(\n",
        "            y_true  = y_train,\n",
        "            y_pred  = predicciones,\n",
        "            squared = False\n",
        "           )\n",
        "    train_scores.append(rmse)\n",
        "\n",
        "    # Error de validación cruzada\n",
        "    scores = cross_val_score(\n",
        "                estimator = modelo,\n",
        "                X         = X_train_prep,\n",
        "                y         = y_train,\n",
        "                scoring   = 'neg_root_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "    # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
        "    cv_scores.append(-1*scores.mean())\n",
        "\n",
        "# Gráfico con la evolución de los errores\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
        "ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
        "ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),\n",
        "        marker='o', color = \"red\", label=\"min score\")\n",
        "ax.set_ylabel(\"root_mean_squared_error\")\n",
        "ax.set_xlabel(\"n_estimators\")\n",
        "ax.set_title(\"Evolución del cv-error vs número árboles\")\n",
        "plt.legend();\n",
        "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmin(cv_scores)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT50QjijASyD"
      },
      "source": [
        "Acorde a las dos métricas utilizadas, el valor óptimo de max_features está cerca de 140."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBk0rGUNASyE",
        "outputId": "426d88e1-2571-4ef4-e0f9-312462b8ac98"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo final\n",
        "# ==============================================================================\n",
        "modelo_final = grid.best_estimator_ #$ sacar el mejor modelo\n",
        "predicciones = modelo.predict(X = X_test_prep) # realizo prdiccion\n",
        "rmse = mean_squared_error(\n",
        "        y_true  = y_test,\n",
        "        y_pred  = predicciones,\n",
        "        squared = False\n",
        "       )\n",
        "print(f\"El error (rmse) de test es: {np.sqrt(rmse)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "QyBimgZgASyE",
        "outputId": "94a3abbe-4604-4351-e28a-527f4cb5ce52"
      },
      "outputs": [],
      "source": [
        "# Predicciones\n",
        "# ==============================================================================\n",
        "predicciones = modelo_final.predict(X_train_prep)\n",
        "observado = y_train\n",
        "\n",
        "# Gráfico del modelo\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "\n",
        "plt.scatter(observado,predicciones)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8yIen71ASyE"
      },
      "source": [
        "## Metodos de Clasificacion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAzQ3yJYASyE"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkDxAsBwASyE"
      },
      "source": [
        "### Descriptivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKP5BXOXASyE",
        "outputId": "cfada660-89b2-4ec5-cb30-8087d12ee415"
      },
      "outputs": [],
      "source": [
        "# Distribucion variable objetivo\n",
        "dist_Y_clas = (datos['nueva_construccion'].value_counts() / len(datos)) * 100\n",
        "print(dist_Y_clas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "n0Am98h_ASyE",
        "outputId": "ab093619-09c7-4ba5-d36b-8f810c610f19"
      },
      "outputs": [],
      "source": [
        "# Gráfico\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
        "\n",
        "sns.violinplot(\n",
        "        x     = 'nueva_construccion',\n",
        "        y     = 'precio',\n",
        "        data  = datos,\n",
        "        #color = \"white\",\n",
        "        ax    = ax\n",
        "    )\n",
        "\n",
        "ax.set_title('Distribución del precio según si es nueva construcción');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_mxEqfQASyF"
      },
      "source": [
        "### Preparación variable objetivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V6lTWhtASyF"
      },
      "outputs": [],
      "source": [
        "# transformar la variable objetivo debido a que viene en texto\n",
        "datos['nueva_construccion'] = datos['nueva_construccion'].map({'No': 0, 'Yes': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UuLUWsGASyF"
      },
      "source": [
        "### División train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96V8-_ZgASyF",
        "outputId": "824592ff-5eb9-49d4-acd5-23777c371873"
      },
      "outputs": [],
      "source": [
        "datos.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WMRDPrkASyF"
      },
      "outputs": [],
      "source": [
        "# Reparto de datos en train y test\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                        datos.drop('nueva_construccion', axis = 'columns'), # X\n",
        "                                        datos['nueva_construccion'], # y\n",
        "                                        train_size   = 0.8, # % de datos para la particion de datos de entrenamiento\n",
        "                                        random_state = 1234,\n",
        "                                        shuffle      = True\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ZApTHQbvASyF",
        "outputId": "4e4d01e7-11ba-4851-891a-a61caf61b6f2"
      },
      "outputs": [],
      "source": [
        "# X de la Partición de entrenamiento\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLtrN8MaASyF",
        "outputId": "4b09219e-f51e-43a3-927e-61031d867b9e"
      },
      "outputs": [],
      "source": [
        "#Y de la Partición de entrenamento\n",
        "participacion_entrenamiento = (y_train.value_counts() / len(y_train)) * 100\n",
        "print(participacion_entrenamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQFzOAJOASyG",
        "outputId": "bba76c75-c0c1-4284-e9fa-38ab2f27f136"
      },
      "outputs": [],
      "source": [
        "#Partición de test\n",
        "participacion_test = (y_test.value_counts() / len(y_test)) * 100\n",
        "print(participacion_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_S_jACrASyG"
      },
      "source": [
        "### Preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n708raIhASyG"
      },
      "source": [
        "#### Pipeline y ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2t9ABbIASyG"
      },
      "outputs": [],
      "source": [
        "# Selección de las variables por tipo\n",
        "# ==============================================================================\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "# Se estandarizan las columnas numéricas y se hace one-hot-encoding de las\n",
        "# columnas cualitativas. Para mantener las columnas a las que no se les aplica\n",
        "# ninguna transformación se tiene que indicar remainder='passthrough'.\n",
        "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list() # vars numericas\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list() # vars categoricas\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "                   [('scale', StandardScaler(), numeric_cols), # vars numericas las voy estadarizar\n",
        "                    ('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)], # las vars categoricas voy onehotencoding\n",
        "                remainder='passthrough')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQTdZ0JHASyG"
      },
      "outputs": [],
      "source": [
        "X_train_prep = preprocessor.fit_transform(X_train) # entrenar y aplicar\n",
        "X_test_prep  = preprocessor.transform(X_test) # aplicar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Z3-duNDrASyG",
        "outputId": "c4911025-f2ff-4a0a-8666-73409987efa2"
      },
      "outputs": [],
      "source": [
        "# Convertir el output en dataframe y añadir el nombre de las columnas\n",
        "# ==============================================================================\n",
        "encoded_cat = preprocessor.named_transformers_['onehot'].get_feature_names_out(cat_cols)\n",
        "nombre_columnas = np.concatenate([numeric_cols, encoded_cat])\n",
        "X_train_prep = preprocessor.transform(X_train)\n",
        "X_train_prep = pd.DataFrame(X_train_prep, columns=nombre_columnas)\n",
        "X_test_prep = pd.DataFrame(X_test_prep, columns=nombre_columnas)\n",
        "X_train_prep.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "eU0jirhSASyG",
        "outputId": "e724f69b-5cc3-42cc-e68e-696f22aedf2e"
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "set_config(display='diagram')\n",
        "\n",
        "preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qafLdL6ASyH"
      },
      "source": [
        "### Ajustar un modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s8La3UcASyH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier # modelo classifier\n",
        "from sklearn.tree import plot_tree # grafica de modelo\n",
        "from sklearn.tree import export_graphviz # grafica de modelo\n",
        "from sklearn.tree import export_text # grafica de modelo\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score # metrica aciertos/totalobservaciones\n",
        "from sklearn.metrics import confusion_matrix # matriz con evaluacion de predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6N91NZmASyH"
      },
      "source": [
        "#### Regresion Logistica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "T6roqgfmASyH",
        "outputId": "9508c5b7-2d23-4ed4-edde-71c1e8fc081f"
      },
      "outputs": [],
      "source": [
        "# Definir los modelos que deseas probar\n",
        "# ==============================================================================\n",
        "\n",
        "modelo_reglog = LogisticRegression(penalty='none') # inciar modelo\n",
        "modelo_reglog.fit(X = X_train_prep, y = y_train) # ajustamos a datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVpsNS2IASyJ",
        "outputId": "a8ce66a1-b2d1-4cc9-ae03-5976dffa72e4"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "predicciones = modelo_reglog.predict(X = X_test_prep,) # obtener predicciones\n",
        "\n",
        "print(\"Matriz de confusión\")\n",
        "print(\"-------------------\")\n",
        "confusion_matrix(\n",
        "    y_true    = y_test,\n",
        "    y_pred    = predicciones\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJeqqm9VASyJ",
        "outputId": "457a6d7e-1d19-405e-bd4c-ce8c7dafc340"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "# El modelo inicial es capaz de predecir correctamente un % de las observaciones del conjunto de test.\n",
        "\n",
        "accuracy = accuracy_score(\n",
        "            y_true    = y_test,\n",
        "            y_pred    = predicciones,\n",
        "            normalize = True\n",
        "           )\n",
        "print(f\"El accuracy de test es: {100 * np.round(accuracy,3)} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVOVYLgTASyJ"
      },
      "outputs": [],
      "source": [
        "# Predicción de probabilidades\n",
        "#-------------------------------------------------------------------------------\n",
        "predicciones = modelo_reglog.predict_proba(X_test_prep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Tb1SsrEXASyJ",
        "outputId": "5a390f9f-c2a8-400f-afb3-5c296f18a9b5"
      },
      "outputs": [],
      "source": [
        "# Clasificación empleando la clase de mayor probabilidad\n",
        "# ------------------------------------------------------------------------------\n",
        "df_predicciones = pd.DataFrame(data=predicciones, columns=['0', '1'])\n",
        "df_predicciones['clasificacion_default_0_5'] = np.where(df_predicciones['0'] > df_predicciones['1'], 0, 1)\n",
        "# revisar encabezado del resultado\n",
        "df_predicciones.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VW8yvYkhASyJ",
        "outputId": "37df9a8b-7ebc-4f9f-9309-5973c6c0f3f9"
      },
      "outputs": [],
      "source": [
        "# Clasificación final empleando un threshold de 0.8 para la clase 1.\n",
        "# ------------------------------------------------------------------------------\n",
        "df_predicciones['clasificacion_custom_0_8'] = np.where(df_predicciones['1'] > 0.8, 1, 0) # np.where son el ifelse de numpy para utilizar threshold : umbral = 0.8\n",
        "df_predicciones.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OoFgWhc3ASyK",
        "outputId": "270a0d29-24e1-4d66-caeb-b866f2cf6694"
      },
      "outputs": [],
      "source": [
        "# Clasificación final empleando un threshold de 0.8 para la clase 1.\n",
        "# ------------------------------------------------------------------------------\n",
        "df_predicciones['clasificacion_0_1'] = np.where(df_predicciones['1'] > 0.1, 1, 0) # np.where son el ifelse de numpy para utilizar threshold\n",
        "# Comparar resultados con otros umbrales.\n",
        "# ------------------------------------------------------------------------------\n",
        "pd.crosstab(df_predicciones.clasificacion_default_0_5,df_predicciones.clasificacion_0_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvzW5atSASyK"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "46tIOVT3ASyK",
        "outputId": "c5667a0a-8d06-4b01-ee24-9a301a9cd367"
      },
      "outputs": [],
      "source": [
        "# Creación del modelo\n",
        "# ------------------------------------------------------------------------------\n",
        "modelo_cart = DecisionTreeClassifier(\n",
        "    # hiperparametros\n",
        "            max_depth         = 5,  # maxima profundidad\n",
        "            criterion         = 'gini', # criterio\n",
        "            random_state      = 123 #semilla aleatoria para uso academico\n",
        "          )\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "# ------------------------------------------------------------------------------\n",
        "modelo_cart.fit(X_train_prep, y_train) # aqui se entrenó el modelo con los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLR3YcbIASyK",
        "outputId": "0a1d1a17-b25d-49b4-fc07-1d8fed28badf"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "predicciones = modelo_cart.predict(X = X_test_prep,) # obtener predicciones\n",
        "\n",
        "print(\"Matriz de confusión\")\n",
        "print(\"-------------------\")\n",
        "confusion_matrix(\n",
        "    y_true    = y_test,\n",
        "    y_pred    = predicciones\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1DPN15NASyK",
        "outputId": "74facb72-e5fc-499d-a023-25f0f802808e"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "# El modelo inicial es capaz de predecir correctamente un % de las observaciones del conjunto de test.\n",
        "\n",
        "accuracy = accuracy_score(\n",
        "            y_true    = y_test,\n",
        "            y_pred    = predicciones,\n",
        "            normalize = True\n",
        "           )\n",
        "print(f\"El accuracy de test es: {100 * np.round(accuracy,3)} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JttsBEMQo7nX"
      },
      "source": [
        "* Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljeL3gQYpMD-"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uDUEUMXbpR12",
        "outputId": "c6de5d4f-48d4-4197-c44c-220bc9974e6e"
      },
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_prep, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuB9e6r-o7CI",
        "outputId": "332262cd-7c53-466c-d780-f0e17d0f637a"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "predicciones = gnb.predict(X = X_test_prep,) # obtener predicciones\n",
        "\n",
        "print(\"Matriz de confusión\")\n",
        "print(\"-------------------\")\n",
        "confusion_matrix(\n",
        "    y_true    = y_test,\n",
        "    y_pred    = predicciones\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fS7-69hpd5H",
        "outputId": "06af961e-b2e4-4d21-8141-efff28738bcc"
      },
      "outputs": [],
      "source": [
        "# Error de test del modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "# El modelo inicial es capaz de predecir correctamente un % de las observaciones del conjunto de test.\n",
        "\n",
        "accuracy = accuracy_score(\n",
        "            y_true    = y_test,\n",
        "            y_pred    = predicciones,\n",
        "            normalize = True\n",
        "           )\n",
        "print(f\"El accuracy de test es: {100 * np.round(accuracy,3)} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6pCwQsEASyL"
      },
      "source": [
        "#### Entrenamiento automatico de varios modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_VCIRdJASyL"
      },
      "outputs": [],
      "source": [
        "# Definir los modelos que deseas probar\n",
        "# ==============================================================================\n",
        "\n",
        "models = [\n",
        "    ('RandomForest', RandomForestClassifier(),\n",
        "     {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 5, 10, 20]}),\n",
        "\n",
        "    ('GradientBoosting', GradientBoostingClassifier(),\n",
        "     {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2],\n",
        "      'max_depth': [3, 5, 10]}),\n",
        "\n",
        "    ('SVM', SVC(), # support vector machine classifier\n",
        "     {'C': np.logspace(-3, 3, 7), 'kernel': ['linear', 'rbf']})\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni02C7giASyL"
      },
      "outputs": [],
      "source": [
        "# Realizar la búsqueda aleatoria y ajustar los modelos\n",
        "# ==============================================================================\n",
        "results = {}\n",
        "for name, model, params in models:\n",
        "    random_search = RandomizedSearchCV(model, params, n_iter=10, cv=5, random_state=42)\n",
        "    random_search.fit(X_train_prep, y_train)\n",
        "\n",
        "    y_pred = random_search.predict(X_test_prep)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'best_params': random_search.best_params_,\n",
        "        'best_score': random_search.best_score_,\n",
        "        'test_accuracy': accuracy\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdiDBSPTASyL",
        "outputId": "37743c29-f42b-47d9-aaf9-34f54482bd8e"
      },
      "outputs": [],
      "source": [
        "# Imprimir los resultados de entrenamiento de los diferentes modelos\n",
        "for model_name, result in results.items():\n",
        "    print(f'Modelo: {model_name}')\n",
        "    print(f'Mejores hiperparámetros: {result[\"best_params\"]}')\n",
        "    print(f'Accuracy en el conjunto de entrenamiento: {result[\"best_score\"]:.3f}')\n",
        "    print(f'Accuracy en el conjunto de prueba: {result[\"test_accuracy\"]:.3f}')\n",
        "    print('-------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1eYHP6-ASyL"
      },
      "source": [
        "## Metodos de Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd5xyxP1ASyL"
      },
      "outputs": [],
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "KfXUGeHoASyL",
        "outputId": "56a46cac-b2bd-4eed-c957-a8f26398fa41"
      },
      "outputs": [],
      "source": [
        "# Simulación de datos porque no tenemos datos (podríamos cargarlos)\n",
        "# ==============================================================================\n",
        "X, y = make_blobs(\n",
        "        n_samples    = 300,\n",
        "        n_features   = 2,\n",
        "        centers      = 4,\n",
        "        cluster_std  = 0.60,\n",
        "        shuffle      = True,\n",
        "        random_state = 0\n",
        "       )\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
        "ax.scatter(\n",
        "    x = X[:, 0],\n",
        "    y = X[:, 1],\n",
        "    c = 'white',\n",
        "    marker    = 'o',\n",
        "    edgecolor = 'black',\n",
        ")\n",
        "ax.set_title('Datos simulados');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NGoYIw2ASyM"
      },
      "source": [
        "***Modelo Kmeans***\n",
        "\n",
        "Con la clase sklearn.cluster.KMeans de Scikit-Learn se pueden entrenar modelos de clustering utilizando el algoritmo k-means. Entre sus parámetros destacan:\n",
        "\n",
        "* n_clusters: determina el número  K\n",
        "  de clusters que se van a generar.\n",
        "* init: estrategia para asignar los centroides iniciales. Por defecto se emplea 'k-means++', una estrategia que trata de alejar los centroides lo máximo posible facilitando la convergencia. Sin embargo, esta estrategia puede ralentizar el proceso cuando hay muchos datos, si esto ocurre, es mejor utilizar 'random'.\n",
        "* n_init: determina el número de veces que se va a repetir el proceso, cada vez con una asignación aleatoria inicial distinta. Es recomendable que este último valor sea alto, entre 10-25, para no obtener resultados subóptimos debido a una iniciación poco afortunada del proceso.\n",
        "* max_iter: número máximo de iteraciones permitidas.\n",
        "* random_state: semilla para garantizar la reproducibilidad de los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw3XD8DnASyM"
      },
      "outputs": [],
      "source": [
        "# Escalado de datos (procesamiento a datos numericos)\n",
        "# ==============================================================================\n",
        "X_scaled = scale(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "pV2pRuIFASyM",
        "outputId": "d060ce06-3af3-4414-babf-764aa82c381c"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "# ==============================================================================\n",
        "\n",
        "modelo_kmeans = KMeans(\n",
        "    n_clusters=4,\n",
        "    n_init=25,\n",
        "    random_state=123\n",
        "    ) #objeto modelo\n",
        "modelo_kmeans.fit(X=X_scaled)# ajuste del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhiiu5yZASyM"
      },
      "source": [
        "El objeto devuelto por KMeans() contiene entre otros datos: la media de cada una de las variables para cada cluster (cluster_centers_), es decir, los centroides. Un vector indicando a qué cluster se ha asignado cada observación (.labels_) y la suma total de cuadrados internos de todos los clusters (.inertia_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWGU34P-ASyM"
      },
      "source": [
        "***Número de clusters***\n",
        "\n",
        "Al tratarse de una simulación, se conoce el verdadero número de grupos (4) y a cuál de ellos pertenece cada observación. Esto no sucede en la mayoría de casos prácticos, pero es útil como ejemplo ilustrativo cómo funciona K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG-dqYKpASyM"
      },
      "outputs": [],
      "source": [
        "# Clasificación con el modelo kmeans\n",
        "# ==============================================================================\n",
        "y_predict = modelo_kmeans.predict(X=X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "K_M-MQhWASyM",
        "outputId": "e7e3894e-71c2-43b4-c9f6-524bfbfd062c"
      },
      "outputs": [],
      "source": [
        "# Representación gráfica: grupos originales vs clusters creados\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Grupos originales\n",
        "for i in np.unique(y):\n",
        "    ax[0].scatter(\n",
        "        x = X_scaled[y == i, 0],\n",
        "        y = X_scaled[y == i, 1],\n",
        "        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n",
        "        marker    = 'o',\n",
        "        edgecolor = 'black',\n",
        "        label= f\"Grupo {i}\"\n",
        "    )\n",
        "\n",
        "ax[0].set_title('Clusters generados por Kmeans')\n",
        "ax[0].legend();\n",
        "\n",
        "for i in np.unique(y_predict):\n",
        "    ax[1].scatter(\n",
        "        x = X_scaled[y_predict == i, 0],\n",
        "        y = X_scaled[y_predict == i, 1],\n",
        "        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n",
        "        marker    = 'o',\n",
        "        edgecolor = 'black',\n",
        "        label= f\"Cluster {i}\"\n",
        "    )\n",
        "\n",
        "ax[1].scatter(\n",
        "    x = modelo_kmeans.cluster_centers_[:, 0],\n",
        "    y = modelo_kmeans.cluster_centers_[:, 1],\n",
        "    c = 'black',\n",
        "    s = 200,\n",
        "    marker = '*',\n",
        "    label  = 'centroides'\n",
        ")\n",
        "ax[1].set_title('Clusters generados por Kmeans')\n",
        "ax[1].legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2rHaM81GASyM",
        "outputId": "faeae791-16e9-45fc-c1d8-19831176dcb0"
      },
      "outputs": [],
      "source": [
        "# Matriz de confusión: grupos originales vs clusters creados\n",
        "# ==============================================================================\n",
        "pd.crosstab(y, y_predict, dropna=False, rownames=['grupo_real'], colnames=['cluster'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "2VjGyVOOASyN",
        "outputId": "11b47903-264b-4770-ae93-58cbfcc03d33"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Resultados para K = 2\n",
        "# ==============================================================================\n",
        "y_predict = KMeans(n_clusters=2, n_init=25, random_state=123).fit_predict(X=X_scaled)\n",
        "ax[0].scatter(\n",
        "        x = X_scaled[:, 0],\n",
        "        y = X_scaled[:, 1],\n",
        "        c = y_predict,\n",
        "        #cmap='viridis',\n",
        "        marker    = 'o',\n",
        "        edgecolor = 'black'\n",
        "    )\n",
        "ax[0].set_title('KMeans K=2');\n",
        "\n",
        "# Resultados para K = 6\n",
        "# ==============================================================================\n",
        "y_predict = KMeans(n_clusters=6, n_init=25, random_state=123).fit_predict(X=X_scaled)\n",
        "ax[1].scatter(\n",
        "        x = X_scaled[:, 0],\n",
        "        y = X_scaled[:, 1],\n",
        "        c = y_predict,\n",
        "        #cmap='viridis',\n",
        "        marker    = 'o',\n",
        "        edgecolor = 'black'\n",
        "    )\n",
        "ax[1].set_title('KMeans K=6');\n",
        "\n",
        "# Resultados para K = 10\n",
        "# ==============================================================================\n",
        "y_predict = KMeans(n_clusters=10, n_init=25, random_state=123).fit_predict(X=X_scaled)\n",
        "ax[2].scatter(\n",
        "        x = X_scaled[:, 0],\n",
        "        y = X_scaled[:, 1],\n",
        "        c = y_predict,\n",
        "        #cmap='viridis',\n",
        "        marker    = 'o',\n",
        "        edgecolor = 'black'\n",
        "    )\n",
        "ax[2].set_title('KMeans K=10');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtCPS9bMASyN"
      },
      "source": [
        "Una forma sencilla de estimar el número K óptimo de clusters cuando no se dispone de esta información, es aplicar el algoritmo de K-means para un rango de valores de K e identificar aquel valor a partir del cual la reducción en la suma total de varianza intra-cluster (inertia) deja de ser sustancial. A esta estrategia se la conoce como método del codo o elbow method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "KnsZAd6CASyN",
        "outputId": "a42cf160-f6df-4ae9-e45f-c595cda4774c"
      },
      "outputs": [],
      "source": [
        "# Método elbow para identificar el número óptimo de clusters\n",
        "# ==============================================================================\n",
        "range_n_clusters = range(1, 15)\n",
        "inertias = []\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    modelo_kmeans = KMeans(\n",
        "                        n_clusters   = n_clusters,\n",
        "                        n_init       = 20,\n",
        "                        random_state = 123\n",
        "                    )\n",
        "    modelo_kmeans.fit(X_scaled)\n",
        "    inertias.append(modelo_kmeans.inertia_)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
        "ax.plot(range_n_clusters, inertias, marker='o')\n",
        "ax.set_title(\"Evolución de la varianza intra-cluster total\")\n",
        "ax.set_xlabel('Número clusters')\n",
        "ax.set_ylabel('Intra-cluster (inertia)');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "OEpevK7rASyN",
        "outputId": "c9666fc9-33e3-4175-eaa7-e15352c716b9"
      },
      "outputs": [],
      "source": [
        "# Método silhouette para identificar el número óptimo de clusters\n",
        "# ==============================================================================\n",
        "range_n_clusters = range(2, 15)\n",
        "valores_medios_silhouette = []\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    modelo_kmeans = KMeans(\n",
        "                        n_clusters   = n_clusters,\n",
        "                        n_init       = 20,\n",
        "                        random_state = 123\n",
        "                    )\n",
        "    cluster_labels = modelo_kmeans.fit_predict(X_scaled)\n",
        "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "    valores_medios_silhouette.append(silhouette_avg)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
        "ax.plot(range_n_clusters, valores_medios_silhouette, marker='o')\n",
        "ax.set_title(\"Evolución de media de los índices silhouette\")\n",
        "ax.set_xlabel('Número clusters')\n",
        "ax.set_ylabel('Media índices silhouette');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdeiZxD02pLf"
      },
      "source": [
        "## Bibliografía"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYXKNa5BASyN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTifdGxy2qzu"
      },
      "source": [
        "\n",
        "* https://www.cienciadedatos.net/documentos/py06_machine_learning_python_scikitlearn.html\n",
        "* https://www.cienciadedatos.net/documentos/py08_random_forest_python.html\n",
        "* https://www.cienciadedatos.net/documentos/py10-regresion-lineal-python.html\n",
        "* https://www.cienciadedatos.net/documentos/py20-clustering-con-python.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f128fe3ab5917b826f85fed29f7e6d5cdf9b8c96daddc94321e327dfff04eefa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
