{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53405b9",
   "metadata": {},
   "source": [
    "\n",
    "# Transformers Extendidos: Visualización, Fine-Tuning y Encoder desde Cero (TensorFlow)\n",
    "\n",
    "Este notebook amplía los fundamentos del modelo Transformer incluyendo:\n",
    "\n",
    "1. Visualización de atención de un modelo preentrenado.\n",
    "2. Fine-tuning completo con métricas y validación.\n",
    "3. Implementación desde cero de un **Encoder Transformer** con TensorFlow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894fdc0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Visualización de Atención\n",
    "\n",
    "Usamos `DistilBERT` de Hugging Face para obtener los pesos de atención.\n",
    "\n",
    "Requiere:\n",
    "```bash\n",
    "pip install transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda03787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\", output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "text = \"The transformer model changed deep learning forever.\"\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # tuple of (layers, batch, heads, seq_len, seq_len)\n",
    "\n",
    "# Visualizar una cabeza de una capa\n",
    "layer = 0\n",
    "head = 0\n",
    "weights = attentions[layer][0, head].numpy()\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(weights, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
    "plt.title(\"Atención Layer 0 - Head 0\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c57a2d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Fine-Tuning Completo (DistilBERT para clasificación binaria)\n",
    "\n",
    "Incluye:\n",
    "- Tokenización\n",
    "- Entrenamiento con validación\n",
    "- Visualización de métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Cargar dataset\n",
    "ds = load_dataset(\"imdb\", split=\"train[:2000]\").train_test_split(test_size=0.2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "ds.set_format(type='tensorflow', columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": ds[\"train\"][\"input_ids\"], \"attention_mask\": ds[\"train\"][\"attention_mask\"]},\n",
    "    ds[\"train\"][\"label\"]\n",
    ")).batch(16)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": ds[\"test\"][\"input_ids\"], \"attention_mask\": ds[\"test\"][\"attention_mask\"]},\n",
    "    ds[\"test\"][\"label\"]\n",
    ")).batch(16)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3cb7c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Implementación de un Encoder Transformer desde Cero\n",
    "\n",
    "Componentes:\n",
    "- MultiHead Attention\n",
    "- Feed Forward\n",
    "- Positional Encoding\n",
    "- Layer Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.mha(x, x, x)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd54cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = tf.random.uniform((1, 50, 128))\n",
    "encoder = EncoderLayer(d_model=128, num_heads=4, dff=512)\n",
    "output = encoder(sample, training=False)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f1633",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Aplicaciones Prácticas con Transformers\n",
    "\n",
    "A continuación se exploran tareas específicas que aprovechan arquitecturas Transformer:\n",
    "\n",
    "- Pregunta-Respuesta (QA)\n",
    "- Resumen Automático (Summarization)\n",
    "- Generación de Texto (Text Generation)\n",
    "- Traducción Automática (Translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61463a7",
   "metadata": {},
   "source": [
    "\n",
    "### Pregunta-Respuesta\n",
    "\n",
    "Usamos `AutoModelForQuestionAnswering` para responder preguntas sobre un contexto dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "contexto = \"Transformers are deep learning models that use self-attention mechanisms to model relationships in sequential data.\"\n",
    "pregunta = \"What do transformers use to model relationships?\"\n",
    "\n",
    "resultado = qa(question=pregunta, context=contexto)\n",
    "print(f\"Respuesta: {resultado['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68813284",
   "metadata": {},
   "source": [
    "\n",
    "### Resumen Automático\n",
    "\n",
    "Usamos `facebook/bart-large-cnn` o `t5-small` para sintetizar textos largos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resumidor = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "texto_largo = '''\n",
    "Transformers have transformed deep learning and NLP. They use attention mechanisms to relate different positions of a sequence and have enabled models like BERT, GPT, and T5 to achieve state-of-the-art results in tasks such as translation, question answering, and summarization.\n",
    "'''\n",
    "\n",
    "resumen = resumidor(texto_largo, max_length=50, min_length=20, do_sample=False)\n",
    "print(\"Resumen:\", resumen[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be3368",
   "metadata": {},
   "source": [
    "\n",
    "### Generación de Texto\n",
    "\n",
    "Utilizamos `GPT2` para generar texto a partir de un prompt inicial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generador = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"Once upon a time, there was a robot that\"\n",
    "resultado = generador(prompt, max_length=50, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df3c66",
   "metadata": {},
   "source": [
    "\n",
    "### Traducción Automática\n",
    "\n",
    "Usamos `Helsinki-NLP/opus-mt-en-es` para traducir del inglés al español.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cac77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traductor = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "frase = \"Deep learning models are becoming increasingly powerful.\"\n",
    "traduccion = traductor(frase)\n",
    "print(\"Traducción:\", traduccion[0]['translation_text'])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
