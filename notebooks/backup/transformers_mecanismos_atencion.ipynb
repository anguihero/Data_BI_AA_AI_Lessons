{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07877fd",
   "metadata": {},
   "source": [
    "\n",
    "# Transformers y Mecanismos de Atención\n",
    "\n",
    "Los **transformers** han revolucionado el procesamiento de lenguaje natural, visión por computador y series temporales, gracias a su capacidad de capturar relaciones a largo plazo y paralelizar el entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamentos\n",
    "\n",
    "- **Atención**: mecanismo que pondera la importancia de cada entrada.\n",
    "- **Self-Attention**: cada elemento presta atención a todos los demás.\n",
    "- **Encoder-Decoder**: estructura usada en traducción y generación de texto.\n",
    "- **Preentrenamiento**: BERT, GPT, T5, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Componentes clave:\n",
    "\n",
    "- Embedding\n",
    "- Positional Encoding\n",
    "- Multi-Head Attention\n",
    "- Feed-Forward Network\n",
    "- Normalización y Residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a985f",
   "metadata": {},
   "source": [
    "\n",
    "## Mecanismo de Atención (Escalar)\n",
    "\n",
    "Dado una consulta (query) \\( q \\), claves (keys) \\( k \\) y valores (values) \\( v \\):\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(q, K, V) = \\text{softmax}\\left(\\frac{q K^T}{\\sqrt{d_k}}\\right) V\n",
    "\\]\n",
    "\n",
    "Este mecanismo permite que el modelo \"atienda\" diferentes partes de la secuencia con distintas intensidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulación simple\n",
    "q = np.array([[1, 0]])    # Query\n",
    "k = np.array([[1, 0], [0, 1]])  # Keys\n",
    "v = np.array([[10, 0], [0, 10]])  # Values\n",
    "\n",
    "# Atención escalar\n",
    "score = q @ k.T\n",
    "weights = np.exp(score) / np.sum(np.exp(score), axis=1, keepdims=True)\n",
    "context = weights @ v\n",
    "\n",
    "print(\"Contexto generado por atención:\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd99ba",
   "metadata": {},
   "source": [
    "\n",
    "## Transformers con TensorFlow y Hugging Face\n",
    "\n",
    "Utilizaremos un modelo preentrenado para clasificación de texto (`distilbert-base-uncased`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q transformers datasets\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar dataset y tokenizer\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:2000]\").train_test_split(test_size=0.2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Convertir a tf.data\n",
    "train = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': dataset['train']['input_ids'], 'attention_mask': dataset['train']['attention_mask']},\n",
    "    dataset['train']['label']\n",
    ")).batch(16)\n",
    "\n",
    "# Cargar modelo\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Entrenar\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f2c95",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Prueba otro modelo como `bert-base-uncased` o `roberta-base`.\n",
    "2. Evalúa sobre el conjunto de prueba.\n",
    "3. Usa un modelo `AutoModelForQuestionAnswering` en un contexto simple.\n",
    "\n",
    "¿Puedes adaptar la arquitectura para clasificación multiclase?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}