{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ac3d31",
   "metadata": {},
   "source": [
    "\n",
    "# Fundamentos de Redes Neuronales\n",
    "\n",
    "Las **redes neuronales artificiales (ANNs)** son modelos inspirados en el cerebro humano. Son capaces de aprender representaciones complejas a partir de datos y se utilizan ampliamente en clasificación, regresión, visión por computador, procesamiento de lenguaje natural y más.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos del notebook:\n",
    "\n",
    "- Entender la estructura básica de una red neuronal.\n",
    "- Explorar diferentes tipos de capas.\n",
    "- Analizar funciones de activación y pérdida.\n",
    "- Comprender el entrenamiento mediante propagación hacia atrás.\n",
    "- Usar técnicas modernas como callbacks, normalización y regularización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ed685",
   "metadata": {},
   "source": [
    "\n",
    "## 1. El Perceptrón\n",
    "\n",
    "El **perceptrón** es la unidad más básica de una red neuronal:\n",
    "\n",
    "\\[\n",
    "y = f(\\sum_{i=1}^{n} w_i x_i + b)\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- \\( w_i \\): pesos\n",
    "- \\( x_i \\): entradas\n",
    "- \\( b \\): sesgo\n",
    "- \\( f \\): función de activación (ej. step, sigmoid, ReLU)\n",
    "\n",
    "Un perceptrón puede clasificar datos linealmente separables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c186d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    return step_function(np.dot(x, w) + b)\n",
    "\n",
    "# Ejemplo\n",
    "x = np.array([1, 0])\n",
    "w = np.array([0.5, -0.6])\n",
    "b = 0.1\n",
    "\n",
    "perceptron(x, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bb0fc",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Arquitectura de una Red Neuronal\n",
    "\n",
    "Una red neuronal típica tiene:\n",
    "\n",
    "- **Capa de entrada**: recibe las características.\n",
    "- **Capas ocultas**: transformaciones intermedias.\n",
    "- **Capa de salida**: produce la predicción final.\n",
    "\n",
    "### Tipos de capas:\n",
    "\n",
    "| Tipo              | Descripción                                  |\n",
    "|-------------------|----------------------------------------------|\n",
    "| Densa (Dense)     | Cada neurona conectada con todas las anteriores. |\n",
    "| Convolucional     | Detecta patrones espaciales (visión).        |\n",
    "| Recurrente        | Procesa secuencias (texto, series temporales).|\n",
    "| Normalización     | Acelera el entrenamiento.                    |\n",
    "| Dropout           | Previene sobreajuste.                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42ac91",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Funciones de Activación\n",
    "\n",
    "### Comunes:\n",
    "\n",
    "| Función    | Fórmula                            | Uso típico         |\n",
    "|------------|------------------------------------|---------------------|\n",
    "| Sigmoid    | \\( \\frac{1}{1 + e^{-x}} \\)      | Clasificación binaria |\n",
    "| ReLU       | \\( \\max(0, x) \\)               | Capas ocultas       |\n",
    "| Tanh       | \\( \\tanh(x) \\)                 | Capas ocultas       |\n",
    "| Softmax    | \\( \\frac{e^{x_i}}{\\sum e^{x_j}} \\) | Clasificación multiclase |\n",
    "\n",
    "Las funciones de activación permiten introducir no linealidad en la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83def8f1",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Función de Costo y Propagación hacia Atrás\n",
    "\n",
    "La **función de pérdida** mide qué tan lejos están las predicciones de los valores reales.\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "- MSE (regresión): \\( \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 \\)\n",
    "- CCE (clasificación): \\( -\\sum y_i \\log(\\hat{y}_i) \\)\n",
    "\n",
    "El entrenamiento consiste en:\n",
    "1. Propagación hacia adelante (forward pass).\n",
    "2. Cálculo de pérdida.\n",
    "3. **Retropropagación** (backpropagation) con gradiente descendente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3f802",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Implementación en Keras\n",
    "\n",
    "Vamos a construir una red neuronal simple con `Keras`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Datos\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(2,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n",
    "print(\"Evaluación:\", model.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0ce03",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Callbacks y Mejores Prácticas\n",
    "\n",
    "### Callbacks comunes:\n",
    "- `EarlyStopping`: detiene el entrenamiento si no mejora.\n",
    "- `ModelCheckpoint`: guarda el mejor modelo.\n",
    "- `TensorBoard`: visualiza métricas.\n",
    "\n",
    "### Otras técnicas:\n",
    "- Regularización (L1/L2)\n",
    "- Dropout\n",
    "- Batch Normalization\n",
    "- Aumento de datos (data augmentation)\n",
    "\n",
    "Estas estrategias ayudan a prevenir sobreajuste y mejorar la generalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdde8a9",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio Final\n",
    "\n",
    "1. Modifica la red para resolver `make_circles`.\n",
    "2. Añade Dropout y observa cómo afecta el resultado.\n",
    "3. Cambia la función de activación a `tanh`.\n",
    "4. Usa `EarlyStopping` para mejorar el tiempo de entrenamiento.\n",
    "\n",
    "Explora cómo los diferentes componentes afectan el desempeño de tu red.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}